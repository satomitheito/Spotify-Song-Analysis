[
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "This page was used to track our progress and keep a log of our contributions to the project. We created this after developing our idea to create a running list of the workflow that was needed to complete the project and to demonstrate how each of us contributed to the project."
  },
  {
    "objectID": "technical-details/progress-log.html#satomi",
    "href": "technical-details/progress-log.html#satomi",
    "title": "Progress log",
    "section": "Satomi",
    "text": "Satomi\nSatomi’s About Me Page\nContribution log:\n\nIndex.qmd Literature Review:\n\nFind 3 academic publications analyzing song popularity using data\nAnalyze publications\nReport Findings on Index.qmd page\n\n\n\nIndex.qmd:\n\nCreate about me page\n\n\n\nData Collection:\n\nDevelop code to access top 100 songs playlist from Rolling Stones top 100 songs\nDevelop code to access spotify api to get data about top 100 songs\nDevelop code to access spotify api to get data about artists of top 100 songs\nProvide comments about process and informative technical markdown cells\nProvide comments about process and informative technical markdown cells\n\n\n\nData Cleansing:\n\nConsolodate all raw data csvs into one dataframe\nUse VADER to conduct sentiment analysis of lyrics for songs\nUse NLP methods to collect other data about lyrics of songs\nFind other quantitative data from columns for future analysis\nWrite all data to a csv file in a processed data folder\nProvide comments about process and informative technical markdown cells\nProvide comments about process and informative technical markdown cells\n\n\n\nSupervised Learning:\n\nConduct classification on ranking of song (top 20 vs not)\nConduct regression on song popularity\nAnalyze results to determine important features contributing to success of song\nProvide comments about process and informative technical markdown cells\n\n\n\nReport:\n\nWrite about key findings from Supervised Learning sections\nProvide informative visualizations\nWrite conclusion\n\n\n\nOverall:\n\nFrequent github pushes\nFrequent github pull requests\nComment code and explain decisions in technical-details files"
  },
  {
    "objectID": "technical-details/progress-log.html#samyu",
    "href": "technical-details/progress-log.html#samyu",
    "title": "Progress log",
    "section": "Samyu",
    "text": "Samyu\nSamyu’s About Me Page\nContribution log:\n\nIndex.qmd Literature Review:\n\nFind 3 academic publications analyzing song popularity using data\nAnalyze publications\nReport Findings on Index.qmd page\n\n\n\nIndex.qmd:\n\nCreate Landing page for website following instructions on page\nCreate about me page\n\n\n\nReport Introduction:\n\nWrite Objective\n\n\n\nData Collection:\n\nDevelop code to access collect lyrics for top 100 songs from Genius API\nProvide comments about process and informative technical markdown cells\n\n\n\nEDA:\n\nDevelop visualizations to analyze distributions of quantitative data\nConduct EDA to understand the characteristics of the most popular songs vs the rest of the songs\nCreate visualizations to show any correlations in the data\nProvide comments about process and informative technical markdown cells\n\n\n\nUnsupervised Learning:\n\nUse clustering algorithms to develop clusters based on quantitative data and understand connection to song ranking or popularity\nAnalyze results to determine the characteristics of popular songs\nDetermine if there are clusters that can then be used to make playlists of similar songs\nProvide comments about process and informative technical markdown cells\n\n\n\nReport:\n\nWrite executive summary\nWrite about key insights provided by EDA\nWrite about key insights provided by unsupervised learning\nProvide informative visualizations\nWrite about Business Implications of key findings\nWrite about Recommendations\n\n\n\nOverall:\n\nFrequent github pushes\nFrequent github pull requests\nComment code and explain decisions in technical-details files"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Music, Lyrics, Hot 100 chart, hit songs, popular music research, Natural Language Processing, Supervised Machine Learning, Unsupervised Machine Learning, Sentiment Analysis\n\n\n\nDecoding Song Success uses data science to uncover insights from the Rolling Stones Top 100 Songs. We bridged NLP and song metadata to analyze features like the characteristics of the lyrics of hits by running sentiment analysis, the popularity of the song, the popularity of the artist, and the genre. We hope to investigate how these features interact to shape the top hits. This website offers a data driven report, informative visualizations, and predictive models to convey the makings of a hit song.\n\n\nMusic is an important part of popular culture with implications in everything from social contexts to consumerism. What makes a song a hit lies in complex emotional and commercial relationships. Analyzing the data about the songs that are considered successful can reveal connections between the lyrical sentiment, the song’s characteristics, and audience preferences. This project highlights these patterns to help inform rising artists and predict commerciall success in the music industry. Some use cases of this analysis include helping songwriters decide whether explicit songs differ significantly from not explicit songs, cluster songs by features to offer recommendations, and help industry professionals predict whether a song will be a hit as they make investment decisions.\n\n\n\nThe overall driving question of this exploration is as follows:\nWhat makes a song more popular with audiences, and how do lyrical and musical factors shape its success?\n\n\n\nAs we conducted our investigation of music data, we aimed to answer the following questions:\n\nHow does lyric sentiment influence a song’s popularity? Do other lyric features (such as length) have an influence?\nDoes lyric sentiment differ across genres, and does the trend between sentiment and popularity change between genres?\nHow does profanity in a song impact it’s sentiment and popularity?\nCan we use unsupervised learning techniques to identify clusters and create playlists of songs that someone may like based on liking a hit song?\nDoes collaboration between artists produce more popular songs? How does individual artist popularity impact this?\n\n\n\n\nThis project provides an in-depth study of the connection between data science and music. As you explore the visualizations, predictive models, and business report, we hop you learn more about what makes a song a hit!\n\n\n\n\n\n\n\n\nHello! I’m Satomi Ito, an aspiring data scientist with a passion for making data-driven decisions. I am currently a Data Science Master’s student at Georgetown University, where I explore the intersection of data-driven insights and impactful solutions.\n\n\n\n\nProgramming Languages: Python, R, MATLAB, SQL\nTechnical Competencies: Predictive Modeling, Statistical Analysis, Data Visualization\nMy background in Cognitive Science equips me with a multidisciplinary approach to technical development, blending human understanding and machine learning.\n\n\n\n\nMy professional journey began with a Bachelor’s degree in Cognitive Science at the University of California, San Diego, where I developed a deep interest in understanding complex systems and patterns.\n\nAfter graduation, I took a year working in research. During my time at a neurology research lab at Zuckerberg General Hospital, I worked on developing predictive models to analyze and interpret neurological data. This experience honed my technical skills and reinforced my passion for solving real-world problems using data.\n\n\n\n\nI approach challenges with a problem-solving mindset, leveraging data to uncover actionable insights. I enjoy collaboration and think that it’s an impact way to grow and learn.\n\n\n\n\nM.S. in Data Science and Analytics, Georgetown University (Current)\nB.S. in Cognitive Science, University of California, San Diego 2019 - 2023\n\n\n\n\nOutside of my academic and professional work, I enjoy: - Climbing: Rope and bouldering - DIY Projects: I am currently building a shelf from scratch.\n\nLinkedIn: Satomi Ito\nGitHub: satomitheito\n\n\n\n\n\n\n\nWelcome to our site! My name is Samyukta (Samyu) Vakkalanka. I am a first year master’s student in the Data Science and Analytics program at Georgetown University. I have experience in government research applying machine learning to cybersecurity and in business intelligence working in marketing analysis. I am interested in academic research in data science, especially as it relates to sustainibility and climate studies.\n\n\n\n\nProgramming Languages: Python, R, MATLAB, HTML/CSS/JavaScript, Java, SQL\nTechnical Competencies: Machine Learning, Natural Language Processing, Data Visualization, Statistics, Applied Mathematics\nMy background in mathematics and statistics uniquely allows me to approach data science problems with a strong foundation in theory, enabling me to develop robust models, apply advanced statistical methodologies, and extract meaningful insights from complex datasets.\n\n\n\n\nMy professional journey began with my undergraduate degrees from the University of Southern California. I studied Data Science and Applied and Computational Mathematics, where I developed a passion for bridging theoretical statistics and the real world applications of data science.\nDuring my time at USC, I completed an internship at Lawrence Livermore National Laboratory. My work consisted of developing novel statistical models and machine learning applications to study industrial control system networks and isolate suspicious behavior. I then worked at Comcast with the marketing and sales channel analytics teams. Here, I used supervised ML models to understand the variance in regions of churn and penetration. I directly continued into my Master’s program, where I now work in the AI Measurement and Development lab studying student computational skills and attention using eye tracking technology.\n\n\n\nMy approach to work combines attention to detail, creativity, and adaptability. I believe breaking down problems into manageable steps is extremely important and allows me to get feedback at every step. I have a strong focus on collaboration and strong communication, where I tailor my presentation of my work for the audience that I am working with.\n\n\n\n\nM.S. in Data Science and Analytics, Georgetown University 2024 - 2026 (expected)\nB.A. in Data Science, University of Southern California 2020 - 2024\nB.A. in Applied and Computational Mathematics, University of Southern California 2020 - 2024\n\n\n\n\n\nMerit Scholarship Award Recipient - Georgetown University 2024-2025\nAthena Hacks Winner - Best Use of Google Cloud 2023\nGoogle Cloud Student Innovator - 2023-2024\nNational Merit Scholarship Finalist and Presidential Scholarship Recipient - 2020-2024\n\n\n\n\nOutside of my academic and professional work, I enjoy: - Music: I have played the piano and violin for 10+ years - Baking: I believe I have officially perfected the art of the chocolate chip cookie (thin, salted, and crispy is the way to go!)\n\n\n\n\nLinkedIn: Samyukta Vakkalanka\nGitHub: samyu-vakkalanka\n\n\n\n\n\n\nTo conduct this literature review, we decided to split articles into two categories: research about the musicality of songs in top charts and papers about the lyrics of songs in top charts. While the first topic has a wide range of approaches, studies related to the lyrics are far rarer. 3 articles were found for each category and the results are reported below.\n\n\n\n\nIn this research article, Interiano et al. uses random forest models to predict whether a song would “make it to the charts” using acoustic features such as mood, danceability, relaxedness, and frequency of male/female singers contributing to the song. They highlighted the trend that songs with fewer male contributors, instrumentals that are considered more “dance-based” music, and brighter sounding songs are strong indicators of the success of a song. The graph below shows this trend over time, with the red dots being songs that are not in the top 100 and blue dots being songs that are in the top 100.\n\n\n\nSource 1 Results\n\n\n\n\n\nKim et al. developed a hierarchical Bayesian logit choice model that takes into account features of the instrumentals of a range of hit songs (such as melody, tempo, harmony, and rhythm) to determine the consumer preference for a specific song. The data was collected through controlled lab tests. They then tried to correlate the resulting preferences to the profits of a song by considering music development costs. Overall, they found that consonant harmony, piano, slow tempo, and irregular rhythm to be more desirable by consumers and to be the most monetarily sustainable choices to include in a song.\n\n\n\nHong analyzed songs that have hit the number 1 spot from 1955 to 2003 and used features such as gender, number, and race of the singers of the song to analyze if artist characteristics contributed to the long-term success of a song (i.e. number of weeks that the song would be at the top of the charts). They used parametric estimation of a log-logistic distribution to find that the presence of Black artists contributed to longer runs at the top of the chart (which they contribute to the success of rap songs). They also concluded that, in groups, the gender of the members of the groups did not actually contribute to the song’s success.\n\n\n\n\n\n\nPettijohn et al. conducted an analysis on the changes in the subjects of the lyrics of Billboard number 1 songs between 1955-2003 using the LIWC program for NLP which produces a series of psychometric features of songs, such as a focus on the future, meaningfulness, comfort, and romance. They then correlated this with US economic conditions collected from various psychology studies regarding the social reaction to US presidents and politics. In the end, they found that songs that were more romantic and comforting, as well as songs with more words per sentence and more mentions of social processes were more common during years that they considered to have more “threatening” conditions.\n\n\n\nStudy 3 in the paper by Nunes et al. considered songs from Billboard’s top 100 singles between 1958-2012 and studied how repetitive they are (such as how many times the chorus is repeated). They determined that more repetitive lyrics correlate strongly with the position at which a song debuts (top 40 or not) and how many weeks it takes for the song to reach the number 1 spot. This understanding of the structure of lyrics is a novel approach to studying the popularity of music and informed some of the questions we asked about our dataset.\n\n\n\nThis report by Tough introduces a new set of features regarding the songwriting procedures of Billboard Hot 100 songs between 2014-2015, including repeating the title in the song, intro length, the presence of archetypes (i.e. the Innocent, the Lover, the Everyman, the Ruler), and song subject. The results include showing that a song in the top 100 is more likely to have an 11-15 second intro, start with a chorus or hook, and have the lover archetype (followed by the warrior archetype). They also find that specific combinations of archetypes (including the warrior/ruler pairing) are more common in the top 100 songs. The below charts show some of these results.\n\n\n\nSource 6 Results - Intro Length\n\n\n\n\n\nSource 6 Results - Intro Length"
  },
  {
    "objectID": "index.html#key-topics",
    "href": "index.html#key-topics",
    "title": "Landing page",
    "section": "",
    "text": "Music, Lyrics, Hot 100 chart, hit songs, popular music research, Natural Language Processing, Supervised Machine Learning, Unsupervised Machine Learning, Sentiment Analysis"
  },
  {
    "objectID": "index.html#project-introduction",
    "href": "index.html#project-introduction",
    "title": "Landing page",
    "section": "",
    "text": "Decoding Song Success uses data science to uncover insights from the Rolling Stones Top 100 Songs. We bridged NLP and song metadata to analyze features like the characteristics of the lyrics of hits by running sentiment analysis, the popularity of the song, the popularity of the artist, and the genre. We hope to investigate how these features interact to shape the top hits. This website offers a data driven report, informative visualizations, and predictive models to convey the makings of a hit song.\n\n\nMusic is an important part of popular culture with implications in everything from social contexts to consumerism. What makes a song a hit lies in complex emotional and commercial relationships. Analyzing the data about the songs that are considered successful can reveal connections between the lyrical sentiment, the song’s characteristics, and audience preferences. This project highlights these patterns to help inform rising artists and predict commerciall success in the music industry. Some use cases of this analysis include helping songwriters decide whether explicit songs differ significantly from not explicit songs, cluster songs by features to offer recommendations, and help industry professionals predict whether a song will be a hit as they make investment decisions.\n\n\n\nThe overall driving question of this exploration is as follows:\nWhat makes a song more popular with audiences, and how do lyrical and musical factors shape its success?\n\n\n\nAs we conducted our investigation of music data, we aimed to answer the following questions:\n\nHow does lyric sentiment influence a song’s popularity? Do other lyric features (such as length) have an influence?\nDoes lyric sentiment differ across genres, and does the trend between sentiment and popularity change between genres?\nHow does profanity in a song impact it’s sentiment and popularity?\nCan we use unsupervised learning techniques to identify clusters and create playlists of songs that someone may like based on liking a hit song?\nDoes collaboration between artists produce more popular songs? How does individual artist popularity impact this?\n\n\n\n\nThis project provides an in-depth study of the connection between data science and music. As you explore the visualizations, predictive models, and business report, we hop you learn more about what makes a song a hit!"
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Landing page",
    "section": "",
    "text": "Hello! I’m Satomi Ito, an aspiring data scientist with a passion for making data-driven decisions. I am currently a Data Science Master’s student at Georgetown University, where I explore the intersection of data-driven insights and impactful solutions.\n\n\n\n\nProgramming Languages: Python, R, MATLAB, SQL\nTechnical Competencies: Predictive Modeling, Statistical Analysis, Data Visualization\nMy background in Cognitive Science equips me with a multidisciplinary approach to technical development, blending human understanding and machine learning.\n\n\n\n\nMy professional journey began with a Bachelor’s degree in Cognitive Science at the University of California, San Diego, where I developed a deep interest in understanding complex systems and patterns.\n\nAfter graduation, I took a year working in research. During my time at a neurology research lab at Zuckerberg General Hospital, I worked on developing predictive models to analyze and interpret neurological data. This experience honed my technical skills and reinforced my passion for solving real-world problems using data.\n\n\n\n\nI approach challenges with a problem-solving mindset, leveraging data to uncover actionable insights. I enjoy collaboration and think that it’s an impact way to grow and learn.\n\n\n\n\nM.S. in Data Science and Analytics, Georgetown University (Current)\nB.S. in Cognitive Science, University of California, San Diego 2019 - 2023\n\n\n\n\nOutside of my academic and professional work, I enjoy: - Climbing: Rope and bouldering - DIY Projects: I am currently building a shelf from scratch.\n\nLinkedIn: Satomi Ito\nGitHub: satomitheito\n\n\n\n\n\n\n\nWelcome to our site! My name is Samyukta (Samyu) Vakkalanka. I am a first year master’s student in the Data Science and Analytics program at Georgetown University. I have experience in government research applying machine learning to cybersecurity and in business intelligence working in marketing analysis. I am interested in academic research in data science, especially as it relates to sustainibility and climate studies.\n\n\n\n\nProgramming Languages: Python, R, MATLAB, HTML/CSS/JavaScript, Java, SQL\nTechnical Competencies: Machine Learning, Natural Language Processing, Data Visualization, Statistics, Applied Mathematics\nMy background in mathematics and statistics uniquely allows me to approach data science problems with a strong foundation in theory, enabling me to develop robust models, apply advanced statistical methodologies, and extract meaningful insights from complex datasets.\n\n\n\n\nMy professional journey began with my undergraduate degrees from the University of Southern California. I studied Data Science and Applied and Computational Mathematics, where I developed a passion for bridging theoretical statistics and the real world applications of data science.\nDuring my time at USC, I completed an internship at Lawrence Livermore National Laboratory. My work consisted of developing novel statistical models and machine learning applications to study industrial control system networks and isolate suspicious behavior. I then worked at Comcast with the marketing and sales channel analytics teams. Here, I used supervised ML models to understand the variance in regions of churn and penetration. I directly continued into my Master’s program, where I now work in the AI Measurement and Development lab studying student computational skills and attention using eye tracking technology.\n\n\n\nMy approach to work combines attention to detail, creativity, and adaptability. I believe breaking down problems into manageable steps is extremely important and allows me to get feedback at every step. I have a strong focus on collaboration and strong communication, where I tailor my presentation of my work for the audience that I am working with.\n\n\n\n\nM.S. in Data Science and Analytics, Georgetown University 2024 - 2026 (expected)\nB.A. in Data Science, University of Southern California 2020 - 2024\nB.A. in Applied and Computational Mathematics, University of Southern California 2020 - 2024\n\n\n\n\n\nMerit Scholarship Award Recipient - Georgetown University 2024-2025\nAthena Hacks Winner - Best Use of Google Cloud 2023\nGoogle Cloud Student Innovator - 2023-2024\nNational Merit Scholarship Finalist and Presidential Scholarship Recipient - 2020-2024\n\n\n\n\nOutside of my academic and professional work, I enjoy: - Music: I have played the piano and violin for 10+ years - Baking: I believe I have officially perfected the art of the chocolate chip cookie (thin, salted, and crispy is the way to go!)\n\n\n\n\nLinkedIn: Samyukta Vakkalanka\nGitHub: samyu-vakkalanka"
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Landing page",
    "section": "",
    "text": "To conduct this literature review, we decided to split articles into two categories: research about the musicality of songs in top charts and papers about the lyrics of songs in top charts. While the first topic has a wide range of approaches, studies related to the lyrics are far rarer. 3 articles were found for each category and the results are reported below.\n\n\n\n\nIn this research article, Interiano et al. uses random forest models to predict whether a song would “make it to the charts” using acoustic features such as mood, danceability, relaxedness, and frequency of male/female singers contributing to the song. They highlighted the trend that songs with fewer male contributors, instrumentals that are considered more “dance-based” music, and brighter sounding songs are strong indicators of the success of a song. The graph below shows this trend over time, with the red dots being songs that are not in the top 100 and blue dots being songs that are in the top 100.\n\n\n\nSource 1 Results\n\n\n\n\n\nKim et al. developed a hierarchical Bayesian logit choice model that takes into account features of the instrumentals of a range of hit songs (such as melody, tempo, harmony, and rhythm) to determine the consumer preference for a specific song. The data was collected through controlled lab tests. They then tried to correlate the resulting preferences to the profits of a song by considering music development costs. Overall, they found that consonant harmony, piano, slow tempo, and irregular rhythm to be more desirable by consumers and to be the most monetarily sustainable choices to include in a song.\n\n\n\nHong analyzed songs that have hit the number 1 spot from 1955 to 2003 and used features such as gender, number, and race of the singers of the song to analyze if artist characteristics contributed to the long-term success of a song (i.e. number of weeks that the song would be at the top of the charts). They used parametric estimation of a log-logistic distribution to find that the presence of Black artists contributed to longer runs at the top of the chart (which they contribute to the success of rap songs). They also concluded that, in groups, the gender of the members of the groups did not actually contribute to the song’s success.\n\n\n\n\n\n\nPettijohn et al. conducted an analysis on the changes in the subjects of the lyrics of Billboard number 1 songs between 1955-2003 using the LIWC program for NLP which produces a series of psychometric features of songs, such as a focus on the future, meaningfulness, comfort, and romance. They then correlated this with US economic conditions collected from various psychology studies regarding the social reaction to US presidents and politics. In the end, they found that songs that were more romantic and comforting, as well as songs with more words per sentence and more mentions of social processes were more common during years that they considered to have more “threatening” conditions.\n\n\n\nStudy 3 in the paper by Nunes et al. considered songs from Billboard’s top 100 singles between 1958-2012 and studied how repetitive they are (such as how many times the chorus is repeated). They determined that more repetitive lyrics correlate strongly with the position at which a song debuts (top 40 or not) and how many weeks it takes for the song to reach the number 1 spot. This understanding of the structure of lyrics is a novel approach to studying the popularity of music and informed some of the questions we asked about our dataset.\n\n\n\nThis report by Tough introduces a new set of features regarding the songwriting procedures of Billboard Hot 100 songs between 2014-2015, including repeating the title in the song, intro length, the presence of archetypes (i.e. the Innocent, the Lover, the Everyman, the Ruler), and song subject. The results include showing that a song in the top 100 is more likely to have an 11-15 second intro, start with a chorus or hook, and have the lover archetype (followed by the warrior archetype). They also find that specific combinations of archetypes (including the warrior/ruler pairing) are more common in the top 100 songs. The below charts show some of these results.\n\n\n\nSource 6 Results - Intro Length\n\n\n\n\n\nSource 6 Results - Intro Length"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This page focuses on using unsupervised machine learning to learn more about the song data and identify what sets the top 20 songs apart from the other. The code is divided into two sections: dimensionality reduction and clustering. In dimensionality reduction, I want to simply high dimensional data while making sure I keep the relationships between the features. These methods allow for better visualizations that will help us find the structure in the dataset. Clustering will let me group songs based on common characteristics. This can be used to find what’s in common between the top performing songs or help users find songs that are a good match with their favorite top 10 or 20 song. Overall, the goal of this page is the find if specific trends or features are indicative of a song’s likelihood of success."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/unsupervised-learning/main.html#introduction-and-motivation",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This page focuses on using unsupervised machine learning to learn more about the song data and identify what sets the top 20 songs apart from the other. The code is divided into two sections: dimensionality reduction and clustering. In dimensionality reduction, I want to simply high dimensional data while making sure I keep the relationships between the features. These methods allow for better visualizations that will help us find the structure in the dataset. Clustering will let me group songs based on common characteristics. This can be used to find what’s in common between the top performing songs or help users find songs that are a good match with their favorite top 10 or 20 song. Overall, the goal of this page is the find if specific trends or features are indicative of a song’s likelihood of success."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#overview-of-methods",
    "href": "technical-details/unsupervised-learning/main.html#overview-of-methods",
    "title": "Unsupervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nGive a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#code",
    "href": "technical-details/unsupervised-learning/main.html#code",
    "title": "Unsupervised Learning",
    "section": "Code",
    "text": "Code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\n\n\nDimensionality Reduction\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\nFor the dimension reduction algorithms, I wanted to alter the genres that I recorded. Previously, only the first genre was recorded. The following functioln records more general genre categories for each of the genres listed for the song. I will then apply one hot encoding to the lists of the simplified genres so I can use it in a clustering algorithm.\nThis code then applies the OHE to the data and removes the non numeric columns. This cleans the data so that we can then apply dimensionality reduction and clustering to the data.\n\ndef map_genres_to_categories(genres):\n    # Define the mapping of specific genres to broader categories\n    genre_mapping = {\n        \"Pop\": [\n            \"pop\", \"social media pop\", \"dance pop\", \"modern country pop\", \"gen z singer-songwriter\", \"colombian pop\", \n            \"modern indie pop\", \"power pop\", \"la pop\", \"singer-songwriter pop\", \"uk pop\", \"alt z\"\n        ],\n        \"Indie/Alternative\": [\n            \"indie pop\", \"modern alternative pop\", \"indie rock\", \"small room\", \"asheville indie\", \"sacramento indie\", \n            \"twee pop\", \"tape club\", \"bubblegrunge\", \"bedroom pop\", \"slacker rock\", \"irish indie rock\", \"irish post-punk\"\n        ],\n        \"Rock\": [\n            \"album rock\", \"blues rock\", \"classic rock\", \"electric blues\", \"hard rock\", \"jam band\", \"southern rock\", \n            \"heartland rock\", \"garage rock\", \"modern blues rock\", \"modern rock\", \"punk blues\", \"noise rock\", \"no wave\"\n        ],\n        \"Country\": [\n            \"classic texas country\", \"contemporary country\", \"country dawn\", \"country road\", \"countrygaze\", \n            \"deep new americana\", \"roots americana\", \"classic oklahoma country\", \"red dirt\"\n        ],\n        \"Hip Hop/Rap\": [\n            \"hip hop\", \"rap\", \"west coast rap\", \"pop rap\", \"melodic rap\", \"houston rap\", \"atl hip hop\", \"trap\", \n            \"trap queen\", \"viral rap\", \"southern hip hop\", \"dfw rap\", \"canadian hip hop\", \"indian underground rap\", \n            \"desi hip hop\", \"malayalam hip hop\", \"irish hip hop\", \"conscious hip hop\"\n        ],\n        \"R&B\": [\n            \"r&b\", \"alternative r&b\", \"uk contemporary r&b\", \"afro r&b\"\n        ],\n        \"Latin\": [\n            \"reggaeton\", \"urbano latino\", \"trap latino\", \"reggaeton chileno\", \"reggaeton colombiano\", \"latin pop\"\n        ],\n        \"Afrobeat/African\": [\n            \"afrobeats\", \"afropop\", \"azonto\", \"nigerian pop\", \"alte\", \"nigerian hip hop\"\n        ],\n        \"Electronic/Experimental\": [\n            \"art pop\", \"metropopolis\", \"ambient folk\", \"freak folk\", \"hyperpop\", \"proto-hyperpop\", \"bubblegum bass\", \n            \"digital hardcore\", \"escape room\", \"experimental pop\", \"experimental hip hop\", \"deconstructed club\", \n            \"electronica\", \"glitch\", \"glitch hop\", \"jazztronica\", \"intelligent dance music\", \"psychedelic hip hop\", \n            \"wonky\", \"indietronica\", \"afrofuturism\", \"transpop\", \"uk alternative pop\", \"crank wave\", \"jersey club\"\n        ],\n        \"K-pop\": [\n            \"k-pop\", \"k-pop girl group\", \"anime\"\n        ],\n        \"Other\": [\n            \"None Listed\"\n        ]\n    }\n\n    categories = []\n    for category, genre_list in genre_mapping.items():\n        # Check if any genre in the song's genre list belongs to this category\n        if any(genre in genres for genre in genre_list):\n            categories.append(category)\n\n    if not categories:  # If no genres match, return \"Other\"\n        categories.append(\"Other\")\n    \n    return categories\n\n\nsongs = pd.read_csv('../../data/processed-data/transformed_data.csv')\nsongs_num = songs[['Genres', 'Song Popularity' ,'Popularity', 'Explicit', 'Total Artists On Song', 'Duration (ms)_transf', 'Followers_transf', 'Lyrics Word Count_transf', 'neg', 'neu', 'pos', 'compound', 'Ranking Group', 'Top 10 Song']]\nsongs_num['Simplified Genre'] = songs_num['Genres'].apply(map_genres_to_categories)\n# One-Hot Encoding the 'Simplified Genre' column\nsongs_num = songs_num.drop(columns=['Ranking Group', 'Top 10 Song'])\nsongs_num_encoded = songs_num['Simplified Genre'].explode().str.get_dummies().groupby(level=0).sum()\n\n# Join the encoded genres back to the original dataframe\nsongs_num = pd.concat([songs_num, songs_num_encoded], axis=1).drop(columns=['Genres', 'Simplified Genre'])\nsongs_num.head()\n\n/var/folders/p2/bg9x0srx2dl9dp4nrwmw4nrc0000gn/T/ipykernel_10132/2605016882.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  songs_num['Simplified Genre'] = songs_num['Genres'].apply(map_genres_to_categories)\n\n\n\n\n\n\n\n\n\nSong Popularity\nPopularity\nExplicit\nTotal Artists On Song\nDuration (ms)_transf\nFollowers_transf\nLyrics Word Count_transf\nneg\nneu\npos\n...\nCountry\nElectronic/Experimental\nHip Hop/Rap\nIndie/Alternative\nK-pop\nLatin\nOther\nPop\nR&B\nRock\n\n\n\n\n0\n0.93\n0.88\nFalse\n1.0\n0.445229\n0.704413\n0.724382\n0.121\n0.454\n0.426\n...\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n1\n0.88\n0.97\nTrue\n1.0\n0.616143\n0.887910\n0.782405\n0.170\n0.719\n0.111\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0.86\n0.81\nTrue\n1.0\n0.262534\n0.565953\n0.726920\n0.085\n0.840\n0.076\n...\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n0.88\n0.94\nTrue\n1.0\n0.280604\n0.820911\n0.739738\n0.017\n0.864\n0.118\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n0.96\n0.96\nFalse\n1.0\n0.417005\n0.981764\n0.701252\n0.150\n0.727\n0.123\n...\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\nPCA (Principal Component Analysis)\nThe following function for graphing the results of the dimensionality reduction algorithm has been adapted from lab 4.2 from DSAN 5000. The function takes in the converted features and plots the first 2 features against each other in a scatter plot and uses a label to color the plots.\n\n# # UTILITY PLOTTING FUNCTION\ndef plot_2D(X,color_vector, dim_red_model):\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature 1 ', ylabel='Feature 2',\n    title= dim_red_model + ' results')\n    ax.grid()\n    plt.show()\n\nThis code takes in the songs_num df without the Song Popularity column (I wanted to use this opportunity to see if clusters naturally occur regardless of the song’s popularity) and the total artist’s on the song (a non standardized column).\n\npca = PCA(n_components=10)  # 95% of variance\npca_result = pca.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song']))\nranking_group = songs['Top 10 Song'].astype('category').cat.codes\nplot_2D(pca_result, ranking_group, 'PCA')\n\n\n\n\n\n\n\n\nI colored the points by whether they were a top 10 song or not. I’m not seeing any clustering with those particular songs, but I am seeing two bigger clusters separated by a diagonal line down the middle, which is interesting. I am not sure what these clusters are indicative of, however.\n\npca1 = PCA(n_components=10)  # 95% of variance\npca_result_1 = pca1.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\nplot_2D(pca_result_1, ranking_group, 'PCA')\n\n\n\n\n\n\n\n\nInterestingly, when I remove the compound score from the sentiment analysis (leaving the positive, negative, neutral scores) I get much clearer clusters (still separated by the diagonal line) with all but 2 of the top songs in the top left cluster.\nThe following code has been adapted from lab 4.2. It takes in a pca model and plots the ratio of explained variance by each principal component. It then plots this.\n\ndef plot_variance_explained(pca):\n    print(\"Variance explained by each principal component:\")\n    print(pca.explained_variance_ratio_[0:10])\n\n    plt.plot(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_, '-o')\n    plt.xlabel('Mumber of components')\n    plt.ylabel('Explained variance ratio')\n    plt.title('Variance Explained by Different PCA Components')\n    plt.show()\n\nplot_variance_explained(pca)\nplot_variance_explained(pca1)\n\nVariance explained by each principal component:\n[0.32070283 0.15954468 0.11482478 0.06969029 0.05864162 0.05514703\n 0.04737073 0.03608452 0.0300553  0.02383462]\n\n\n\n\n\n\n\n\n\nVariance explained by each principal component:\n[0.24156774 0.17060014 0.10147743 0.08326044 0.07891062 0.06986966\n 0.05186226 0.04267764 0.034445   0.03158158]\n\n\n\n\n\n\n\n\n\nThese are two elbow plots showing the ratio of explained variance by each PCA. The first one is for the first model with the elbow point being at 2 components. After this, the explained variance decreases very slowly. For the second one, the elbow point is at either component 2 or 3. Therefore, 2 or 3 components would be optimal for a model.\n\n\nT-SNE\n\nfor perplexity in [5, 10, 30, 50]:\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n    X_tsne = tsne.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song']))\n    plot_2D(X_tsne, ranking_group, f'T-SNE (perplexity={perplexity})')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter running t-sne with several different perplexity values, I’m not seeing very many clear clusters. The best option is the first one with a perplexity of 5. I will try again taking out the compound score to see if that improves it.\n\nfor perplexity in [5, 10, 30, 50]:\n    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n    X_tsne = tsne.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\n    plot_2D(X_tsne, ranking_group, f'T-SNE (perplexity={perplexity})')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above graphs appear to have much clearer clusters. I think the best one is the first one with a perplexity of 5. I see distinct clusters. However, the top-10 songs did not appear to group together.\n\n\nComparison\n\nfig, axes = plt.subplots(1, 2)\n\n# PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\naxes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=ranking_group, alpha=0.5)\naxes[0].set_title('PCA results')\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\n\n# t-SNE\ntsne = TSNE(n_components=2, perplexity=5)\nX_tsne = tsne.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\naxes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=ranking_group, alpha=0.5)\naxes[1].set_title('t-SNE results')\naxes[1].set_xlabel('Feature 1')\naxes[1].set_ylabel('Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI decided to put the two graphs together on the same subplot. This view of both methods shows that T-SNE did a much better job at visualizing clusters. The clusters are closer together and farther apart from each other. However, PCA did a better job of finding the top-10 songs grouped together (other than the 2 outliers) with T-SNE having 1-3 top 10 songs in each cluster.\nIn terms of preseriving the data structure, the fact that PCA kept the top-10 grouping as well as the characteristics of the method itself of projecting data in directions of max variance, I am led to believe that it does a good job of preserving the overall trends and variance but not as good of a job on a local level. This, instead, is where T-SNE does better. This might mean that T-SNE will do a better job showing groupings between songs (perhaps functioning as a recommendation system). Visually, T-SNE has more distinct clusters compared to PCA. However, the axes in PCA are more informative because they show the features produced with the greatest variance. Also, the PCA graph seems to have some underlying linearity which is intersting.\nThe tradeoffs between PCA and T-SNE include the interpretability (with PCA results being meaningful interpretations of the data and T-SNE features being more abstract), dimensionality (PCA can have any number of features while T-SNE is limited to 2-3), and visualization (T-SNE has clearer clusters while PCA’s are more linear) - Source - ChatGPT.\n\n\nK-Means\nK Means clustering is a form of unsupervised learning that looks for the underlying patterns within the data without trying to predict pre determined labels. K Means creates K clusters that group together the points that are closest to the means of the clusters. The goal is that by finding the means of the clusters, data that is very similar to each other will all be grouped together and that there will be separations between the different groups. This is ultimately accomplished by minimizing the sum of squares within each cluster. Usually, we have to find the optimal number of K for the data.\nThe way that it works is by starting out with randomly assigned cluster centroids (means). It then puts each item in the group with it’s closest mean. A new mean is found for the clusters and the points are all reassigned to their nearest centroid. This repeated a given number of times, with the clusters becoming more clear and closer together with each iteration. The method stops when either the centroids stop moving with each iteration or we have hit the maximum number of iterations. The means are calculated using Euclidean distance between all of the input features. The goal is to minimize the sum of squared distances between points and their centroids.\nTo find the optimal number of clusters, we can use the elbow method, where we graph the inertia vs the num clusters and then pick the elbow point, or where the rate of decrease of the inertia slows down significantly. This will give us the optimal number of clusters before we start losing information. Another option is the silhouette score. The score calculates how good of a match each point is with its cluster as opposed to other clusters. A score close to 1 means the point is close to other points in its own cluster and far away from other clusters. A score of 0 means the point is on the boundary between clusters. A score of -1 means a point is in the wrong cluster. The highest average silhouette score is chosen as the optimal number of clusters because it makes the best-defined clusters.\nSource: Geeks for Geeks\nThe following code was adapted from lab 4.2. The methods were altered to only handle kmeans and produce final graphs for pca and t-sne based on the above results to visualize the clusters.\n\n# source: James' lab demonstration\n# function to create different models and fit model to data. Returns model inertia and labels\ndef kmean_fit(k, df):\n    kmeans=KMeans(n_clusters=k)\n    kmeans.fit(df)\n    return kmeans.inertia_, kmeans.labels_\n\n# empty lists to store info about different versions of models to graph\nks=[]\ninertias=[]\nsilhouette_scores = []\n\n# Test different numbers of clusters\nfor k in range(2,20):\n    inertia, labels = kmean_fit(k, songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound'])) # build and fit model\n    ks.append(k) # keep track of number of clusters\n    inertias.append(inertia)\n\n    silhouette_avg = silhouette_score(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']), labels) # calculate silhouette score\n    silhouette_scores.append(silhouette_avg)\n\n# graph num clusters vs inertia\nplt.plot(ks,inertias,\"-o\")\nplt.xlabel(\"Number of Clusters, k\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.show()\n\n# graph num clusters vs silhouette\nplt.plot(ks, silhouette_scores, \"-o\")\nplt.xlabel(\"Number of Clusters, k\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score for Optimal k\")\nplt.show()\n\n# choose optimal nclusters of 2 (highest silhouette, inertia)\nkmeans=KMeans(n_clusters=8)\nkmeans.fit(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\nkmeans_labels = kmeans.predict(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound'])) # predict labels\n\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\nplot_2D(X_pca, kmeans_labels, 'PCA')\n\ntsne = TSNE(n_components=2, perplexity=5)\nX_tsne = tsne.fit_transform(songs_num.drop(columns=['Song Popularity', 'Total Artists On Song', 'compound']))\nplot_2D(X_tsne, kmeans_labels, 'T-SNE')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the silhouette score and elbow graph, I decided to choose 8 clusters as the optimal number of clusters. I think this was a good choice based on the T-SNE Results graph (the PCA graph is also pretty good, but probably could be better). The colors are separated for the most part and the clusters are far away from each other.\n\n# Add the K-Means cluster labels back to the original DataFrame\nsongs['Cluster'] = kmeans_labels\n# Group songs by their clusters and display\nfor cluster_num in sorted(songs['Cluster'].unique()):\n    print(f\"\\nCluster {cluster_num}:\")\n    display(songs[songs['Cluster'] == cluster_num][['Track Name', 'Artists', 'Album', 'Genres', 'Cluster']])\n\n\nCluster 0:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n6\nNasty\nTinashe\nNasty\nalternative r&b, dance pop, metropopolis, pop,...\n0\n\n\n45\nTRYIN' MY HARDEST\nSiR\nHEAVY\nalternative r&b, la pop\n0\n\n\n60\nLove Me JeJe\nTems\nBorn in the Wild\nafro r&b, afrobeats, alte, nigerian pop\n0\n\n\n65\nDENIAL IS A RIVER\nDoechii\nAlligator Bites Never Heal\nalternative r&b\n0\n\n\n69\nLagos Love Story\nAyra Starr\nThe Year I Turned 21\nafrobeats\n0\n\n\n\n\n\n\n\n\nCluster 1:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n2\nA Bar Song (Tipsy)\nShaboozey\nA Bar Song (Tipsy)\nmodern country pop, pop rap\n1\n\n\n3\nEspresso\nSabrina Carpenter\nShort n' Sweet\npop\n1\n\n\n10\nYA YA\nBeyoncé\nCOWBOY CARTER\npop, r&b\n1\n\n\n15\nGenesis.\nRAYE\nGenesis.\nuk contemporary r&b, uk pop\n1\n\n\n19\nGata Only\nFloyyMenor\nGata Only\nreggaeton chileno\n1\n\n\n24\nHISS\nMegan Thee Stallion\nHISS\nhouston rap, pop, rap, trap queen\n1\n\n\n26\nI Had Some Help (Feat. Morgan Wallen)\nPost Malone\nF-1 Trillion\ndfw rap, melodic rap, pop, rap\n1\n\n\n29\nModern Woman\nEliza McLamb\nGoing Through It\ngen z singer-songwriter\n1\n\n\n30\nCompany Culture\nLambrini Girls\nCompany Culture\nNone Listed\n1\n\n\n37\nobsessed\nOlivia Rodrigo\nGUTS (spilled)\npop\n1\n\n\n39\nEnough (Miami)\nCardi B\nEnough (Miami)\npop, rap\n1\n\n\n41\nBon Bon\nFcukers\nBon Bon\nNone Listed\n1\n\n\n50\nActive\nAsake\nActive\nafrobeats, nigerian pop\n1\n\n\n56\nGet In With Me\nBossMan Dlow\nMr Beat The Road\nNone Listed\n1\n\n\n58\nI Love You, I'm Sorry\nGracie Abrams\nThe Secret of Us\nalt z\n1\n\n\n61\nFamily Matters\nDrake\nFamily Matters\ncanadian hip hop, canadian pop, hip hop, pop r...\n1\n\n\n68\nJOYRIDE\nKesha\nJOYRIDE\ndance pop, pop\n1\n\n\n\n\n\n\n\n\nCluster 2:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n46\nBYE BYE\nKim Gordon\nThe Collective\nno wave, noise rock\n2\n\n\n62\nThe Lighthouse\nStevie Nicks\nThe Lighthouse\nheartland rock\n2\n\n\n70\nWanting and Waiting\nThe Black Crowes\nHappiness Bastards\nalbum rock, blues rock, classic rock, electric...\n2\n\n\n88\nOn The Game\nThe Black Keys\nOhio Players\nalternative rock, blues rock, garage rock, ind...\n2\n\n\n\n\n\n\n\n\nCluster 3:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n1\nNot Like Us\nKendrick Lamar\nNot Like Us\nconscious hip hop, hip hop, rap, west coast rap\n3\n\n\n11\nType Shit\nFuture\nWE DON'T TRUST YOU\natl hip hop, hip hop, rap, southern hip hop, trap\n3\n\n\n14\neuphoria\nKendrick Lamar\neuphoria\nconscious hip hop, hip hop, rap, west coast rap\n3\n\n\n27\nTough\nQuavo\nTough\natl hip hop, melodic rap, rap, trap\n3\n\n\n28\nTGIF\nGloRilla\nGLORIOUS\nsouthern hip hop\n3\n\n\n35\nNever Lose Me (feat. SZA & Cardi B)\nFlo Milli\nFine Ho, Stay\nviral rap\n3\n\n\n49\nGet It Sexyy\nSexyy Red\nIn Sexyy We Trust\ntrap queen\n3\n\n\n64\nFine Art\nKNEECAP\nFine Art\nirish hip hop\n3\n\n\n72\nNo One Else (feat. Jeremih)\nLola Brooke\nNo One Else (feat. Jeremih)\ntrap queen\n3\n\n\n84\nKEHLANI (REMIX) [feat. Kehlani]\nJordan Adetunji\nKEHLANI (REMIX) [feat. Kehlani]\nirish hip hop\n3\n\n\n87\nBig Dawgs\nHanumankind\nBig Dawgs\ndesi hip hop, indian underground rap, malayala...\n3\n\n\n92\nADIVINO\nMyke Towers\nADIVINO\nreggaeton, trap latino, urbano latino\n3\n\n\n\n\n\n\n\n\nCluster 4:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n4\nBIRDS OF A FEATHER\nBillie Eilish\nHIT ME HARD AND SOFT\nart pop, pop\n4\n\n\n8\nGirl, so confusing featuring lorde\nCharli xcx\nBrat and it’s completely different but also st...\nart pop, candy pop, metropopolis, pop, uk pop\n4\n\n\n16\nDisease\nLady Gaga\nDisease\nart pop, dance pop, pop\n4\n\n\n25\nVon dutch\nCharli xcx\nBRAT\nart pop, candy pop, metropopolis, pop, uk pop\n4\n\n\n31\nAlways and Forever (feat. Hannah Diamond)\nSOPHIE\nSOPHIE\nart pop, bubblegum bass, deconstructed club, e...\n4\n\n\n33\nFisherrr - Remix\nCash Cobain\nFisherrr (Remix)\njersey club\n4\n\n\n42\nDriver\nSoccer Mommy\nEvergreen\nart pop, bubblegrunge, indie pop, indie rock, ...\n4\n\n\n47\nIn The Wawa (Convinced I Am God)\nLip Critic\nHex Dealer\ndigital hardcore\n4\n\n\n53\nWorld on a String\nJessica Pratt\nHere in the Pitch\nambient folk, art pop, freak folk\n4\n\n\n55\nStarburster\nFontaines D.C.\nRomance\ncrank wave, irish indie rock, irish post-punk\n4\n\n\n57\nLike I Say (I runaway)\nNilüfer Yanya\nMy Method Actor\nart pop, experimental pop, uk alternative pop\n4\n\n\n78\nAjhussi\nFlying Lotus\nSpirit Box\nafrofuturism, alternative hip hop, electronica...\n4\n\n\n91\nBritpop\nA. G. Cook\nBritpop\nbubblegum bass, escape room, hyperpop, proto-h...\n4\n\n\n\n\n\n\n\n\nCluster 5:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n0\nGood Luck, Babe!\nChappell Roan\nGood Luck, Babe!\nindie pop\n5\n\n\n9\nRight Back to It\nWaxahatchee\nTigers Blood\nalabama indie, bubblegrunge, chamber pop, coun...\n5\n\n\n13\nDocket (feat. Bully)\nBlondshell\nDocket (feat. Bully)\nbubblegrunge, indie pop\n5\n\n\n17\nJuna\nClairo\nCharm\nbedroom pop, indie pop, pov: indie\n5\n\n\n52\nNever Need Me\nRachel Chinouriri\nWhat A Devastating Turn of Events\nindie pop, uk pop\n5\n\n\n63\nSoup\nRemi Wolf\nBig Ideas\nindie pop, modern alternative pop\n5\n\n\n67\nBeaches\nbeabadoobee\nThis Is How Tomorrow Moves\nbedroom pop, bubblegrunge, indie pop, pov: indie\n5\n\n\n76\nCan't Be Still\nilluminati hotties\nPOWER\nbubblegrunge\n5\n\n\n79\nCalifornia Highway 99\nThe Softies\nThe Bed I Made\nsacramento indie, tape club, twee pop\n5\n\n\n82\nMilk\nAbby Sage\nThe Rot\nmodern indie pop\n5\n\n\n\n\n\n\n\n\nCluster 6:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n7\nPink Skies\nZach Bryan\nPink Skies\nclassic oklahoma country\n6\n\n\n22\nAngel Of My Dreams\nJADE\nAngel Of My Dreams\nNone Listed\n6\n\n\n23\nShe's Leaving You\nMJ Lenderman\nManning Fireworks\nasheville indie, countrygaze, slacker rock\n6\n\n\n36\nDarkest Hour (Helene Edit)\nEric Church\nDarkest Hour (Helene Edit)\ncontemporary country, country, country road\n6\n\n\n40\nSi Antes Te Hubiera Conocido\nKAROL G\nSi Antes Te Hubiera Conocido\nreggaeton, reggaeton colombiano, urbano latino\n6\n\n\n48\nPerfume\nThe Dare\nWhat's Wrong With New York?\nNone Listed\n6\n\n\n51\nCardinal\nKacey Musgraves\nDeeper Well\nclassic texas country, contemporary country, c...\n6\n\n\n71\nCarousel Horses\nChristian Lee Hutson\nParadise Pop. 10\ncountrygaze, deep new americana\n6\n\n\n74\nMidas\nWunderhorse\nMidas\nNone Listed\n6\n\n\n77\nRadio Wave\nSilverada\nSilverada\nclassic texas country, red dirt, roots americana\n6\n\n\n81\nMasquerade\nOriginal Cast of Stereophonic\nStereophonic (Original Cast Recording)\nNone Listed\n6\n\n\n83\nDammit Randy\nMiranda Lambert\nDammit Randy\ncontemporary country, country, country dawn, c...\n6\n\n\n\n\n\n\n\n\nCluster 7:\n\n\n\n\n\n\n\n\n\nTrack Name\nArtists\nAlbum\nGenres\nCluster\n\n\n\n\n5\nToo Sweet\nHozier\nUnreal Unearth: Unaired\nirish singer-songwriter, modern rock, pop, pov...\n7\n\n\n12\nwe can't be friends (wait for your love)\nAriana Grande\neternal sunshine\npop\n7\n\n\n18\nTaste\nSabrina Carpenter\nShort n' Sweet\npop\n7\n\n\n20\nAPT.\nROSÉ\nAPT.\nk-pop\n7\n\n\n21\nDiet Pepsi\nAddison Rae\nDiet Pepsi\nsocial media pop\n7\n\n\n32\nThe Prophecy\nTaylor Swift\nTHE TORTURED POETS DEPARTMENT: THE ANTHOLOGY\npop\n7\n\n\n34\nHow Sweet\nNewJeans\nHow Sweet\nk-pop, k-pop girl group\n7\n\n\n38\nIgual Que Un Ángel (with Peso Pluma)\nKali Uchis\nORQUÍDEAS\ncolombian pop\n7\n\n\n43\nAnything\nGriff\nVertigo\nalt z, uk pop\n7\n\n\n44\nSaturn\nSZA\nSaturn\npop, r&b, rap\n7\n\n\n54\nCómo Dónde y Cuándo\nShakira\nLas Mujeres Ya No Lloran\ncolombian pop, dance pop, latin pop, pop\n7\n\n\n59\n215634\nElvie Shane\nDamascus\nmodern country pop\n7\n\n\n66\nNew Woman (feat. ROSALÍA)\nLISA\nNew Woman (feat. ROSALÍA)\nk-pop\n7\n\n\n73\nTraining Season\nDua Lipa\nRadical Optimism\ndance pop, pop, uk pop\n7\n\n\n75\nPiece of My Heart (feat. Brent Faiyaz)\nWizkid\nPiece of My Heart (feat. Brent Faiyaz)\nafrobeats, afropop, azonto, nigerian hip hop, ...\n7\n\n\n80\nUse Me\nZach Top\nCold Beer & Country Music\nmodern country pop\n7\n\n\n85\nThat's My Floor\nMagdalena Bay\nImaginal Disk\nla pop\n7\n\n\n86\nIn Front of Me Now\nNada Surf\nMoon Mirror\npower pop\n7\n\n\n89\nAfter Hours\nKehlani\nCRASH\npop, r&b, rap\n7\n\n\n90\nXO (Only If You Say Yes)\nENHYPEN\nROMANCE : UNTOLD\nanime, k-pop boy group\n7\n\n\n93\nBeautiful Things\nBenson Boone\nFireworks & Rollerblades\nsinger-songwriter pop\n7\n\n\n\n\n\n\n\nI decided to display the playlists that would be made from these clusters. Looking at them, I can see that genre had a huge influence on the clusters, but there is also some variation that came from the other features. A manual overview of these playlists make me think that the algorithm did a good job. A lot of these artists (like Sabrina Carpenter, Taylor Swift, Ariana Grande, and DUa Lipa in cluster 7) are often listened to together. Overall, this method was strong.\n\n\n\nClustering Methods\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations.\n\n\n\nDBSCAN\nDBSCAN is also a method for unsupervised learning where labels aren’t provided to the model so it tries to figure out the underlying patterns in the data. DBSCAN stands for density-based spatial clustering of applications with noise. DBSCAN is special because it doesn’t require clear shapes for the clusters and does not require optimization for the number of clusters. It also does a good job with noise. The main concept is the density of points, where we look for spaces that have a high density of p\nSource: Data Camp\n\n\nHierarchical (Agglomerative) Clustering"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#summary-and-interpretation-of-results",
    "href": "technical-details/unsupervised-learning/main.html#summary-and-interpretation-of-results",
    "title": "Unsupervised Learning",
    "section": "Summary and Interpretation of Results",
    "text": "Summary and Interpretation of Results\nSummarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This is the page for exploratory data analysis. After making our processed data files, we wanted to explore the distributions of various features, the correlation between aspects of songs, and the statistical significance of differences between different categories. This will help us understand the data better, look for any preliminary interesting patterns and insights, and make informed decisions about the models we build as we go forward. Specifically, we looked at the song lyrics and the metadata about the music, their relationships, and their contribution onn song popularity and chart rankings. It will also allow us to look for anomalies to make sure that our data is suitable for ML tasks."
  },
  {
    "objectID": "technical-details/eda/main.html#introduction-and-motivation",
    "href": "technical-details/eda/main.html#introduction-and-motivation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This is the page for exploratory data analysis. After making our processed data files, we wanted to explore the distributions of various features, the correlation between aspects of songs, and the statistical significance of differences between different categories. This will help us understand the data better, look for any preliminary interesting patterns and insights, and make informed decisions about the models we build as we go forward. Specifically, we looked at the song lyrics and the metadata about the music, their relationships, and their contribution onn song popularity and chart rankings. It will also allow us to look for anomalies to make sure that our data is suitable for ML tasks."
  },
  {
    "objectID": "technical-details/eda/main.html#overview-of-methods",
    "href": "technical-details/eda/main.html#overview-of-methods",
    "title": "Exploratory Data Analysis",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nThis section uses data visualization, aggregation techniques, and conducts statistical tests to do a preliminary analysis of the data. We used the following methods: - skew, kurtosis: methods to determine the skewness and deviation from normal distribution of the data - MinMaxScaler: method to rescale data to be in the same range as other features in dataset - Ttest_ind: method from scipy.stats to run a t test to check for independence between two groups - f_oneway: method from scipy.stats to run a one-way anova test to check for independce between multiple groups"
  },
  {
    "objectID": "technical-details/eda/main.html#code",
    "href": "technical-details/eda/main.html#code",
    "title": "Exploratory Data Analysis",
    "section": "Code",
    "text": "Code\n\nStep 1: Import data and clean data\n\nImported the data using pandas\nIsolated the rows that I thought might be helpful to analyze for this section of the analysis\nReplaced the null values in genre with “None listed” so I can still use the rest of the data (4 rows)\nChange VADER results into indvidual columns\nDisplayed the results\n\n\nimport pandas as pd\nimport ast\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import f_oneway\n\n\ndf = pd.read_csv('../../data/processed-data/artist_song_masterlist.csv')\ndf.head()\n\n\n\n\n\n\n\n\nTrack ID\nTrack Name\nSong Popularity\nAlbum\nSong Release Date\nDuration (ms)\nArtists\nExplicit\nSong Rank\nArtist ID\n...\nGenre_trap latino\nGenre_trap queen\nGenre_twee pop\nGenre_uk alternative pop\nGenre_uk contemporary r&b\nGenre_uk pop\nGenre_urbano latino\nGenre_viral rap\nGenre_west coast rap\nGenre_wonky\n\n\n\n\n0\n0WbMK4wrZ1wFSty9F7FCgu\nGood Luck, Babe!\n93.0\nGood Luck, Babe!\n2024-04-05\n218423.0\nChappell Roan\nFalse\n1\n7GlBOeep6PqTfFi59PTUUN\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n6AI3ezQ4o3HUoP6Dhudph3\nNot Like Us\n88.0\nNot Like Us\n2024-05-04\n274192.0\nKendrick Lamar\nTrue\n2\n2YZyLoL8N0Wb9xBt1NhZWg\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n2FQrifJ1N335Ljm3TjTVVf\nA Bar Song (Tipsy)\n86.0\nA Bar Song (Tipsy)\n2024-04-12\n171291.0\nShaboozey\nTrue\n3\n3y2cIKLjiOlp1Np37WiUdH\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n2HRqTpkrJO5ggZyyK6NPWz\nEspresso\n88.0\nShort n' Sweet\n2024-08-23\n175459.0\nSabrina Carpenter\nTrue\n4\n74KM79TiuVKeVCqs8QtB0B\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n6dOtVTDdiauQNBQEDOtlAB\nBIRDS OF A FEATHER\n96.0\nHIT ME HARD AND SOFT\n2024-05-17\n210373.0\nBillie Eilish\nFalse\n5\n6qqNVTkY8uBg9cP3Jd7DAH\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 146 columns\n\n\n\n\ndf_cl = df[['Track Name','Song Popularity','Album','Song Release Date','Duration (ms)','Artists','Explicit','Song Rank','Artist ID','Genres','Followers','Popularity','Lyrics','Total Artists On Song','Lyrics Word Count','Sentiment (VADER)']]\ndf_cl.loc[df_cl['Genres'].isna(), 'Genres'] = 'None Listed'\ndf_cl.loc[df_cl['Lyrics'] == \"Lyrics not found!\", 'Lyrics Word Count'] = 0\ndf_cl.loc[df_cl['Lyrics'] == \"Lyrics not found!\", 'Lyrics'] = ' '\ndf_cl = df_cl.dropna(subset='Lyrics')\ndf_cl.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 94 entries, 0 to 97\nData columns (total 16 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Track Name             94 non-null     object \n 1   Song Popularity        94 non-null     float64\n 2   Album                  94 non-null     object \n 3   Song Release Date      94 non-null     object \n 4   Duration (ms)          94 non-null     float64\n 5   Artists                94 non-null     object \n 6   Explicit               94 non-null     bool   \n 7   Song Rank              94 non-null     int64  \n 8   Artist ID              94 non-null     object \n 9   Genres                 94 non-null     object \n 10  Followers              94 non-null     int64  \n 11  Popularity             94 non-null     int64  \n 12  Lyrics                 94 non-null     object \n 13  Total Artists On Song  94 non-null     float64\n 14  Lyrics Word Count      94 non-null     int64  \n 15  Sentiment (VADER)      94 non-null     object \ndtypes: bool(1), float64(3), int64(4), object(8)\nmemory usage: 11.8+ KB\n\n\n\ndf_cl[\"Sentiment (VADER)\"] = df_cl[\"Sentiment (VADER)\"].apply(ast.literal_eval)\n\n# Expand the dictionary into separate columns\nsentiment_df = pd.json_normalize(df_cl[\"Sentiment (VADER)\"])\nsentiment_df = sentiment_df.reset_index(drop=True)\n\n# Combine the expanded columns back with the original DataFrame\ndf_cl = df_cl.reset_index(drop=True)\ndf_cl = pd.concat([df_cl.drop(columns=[\"Sentiment (VADER)\"]), sentiment_df], axis=1)\ndf_cl.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 94 entries, 0 to 93\nData columns (total 19 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Track Name             94 non-null     object \n 1   Song Popularity        94 non-null     float64\n 2   Album                  94 non-null     object \n 3   Song Release Date      94 non-null     object \n 4   Duration (ms)          94 non-null     float64\n 5   Artists                94 non-null     object \n 6   Explicit               94 non-null     bool   \n 7   Song Rank              94 non-null     int64  \n 8   Artist ID              94 non-null     object \n 9   Genres                 94 non-null     object \n 10  Followers              94 non-null     int64  \n 11  Popularity             94 non-null     int64  \n 12  Lyrics                 94 non-null     object \n 13  Total Artists On Song  94 non-null     float64\n 14  Lyrics Word Count      94 non-null     int64  \n 15  neg                    94 non-null     float64\n 16  neu                    94 non-null     float64\n 17  pos                    94 non-null     float64\n 18  compound               94 non-null     float64\ndtypes: bool(1), float64(7), int64(4), object(7)\nmemory usage: 13.4+ KB\n\n\n\n\nUnivariate Analysis\n\nSummary Statistics\nI ran .describe on the dataset to understand the distributiopn of values and decide if it was worth normalizing the data due to the wide range of possibilities for values for several of the columns. Duration, followers, and word count all range from very low to very high values, so it might be worth normalizing these columns before analysis.\n\ndf_cl.describe()\n\n\n\n\n\n\n\n\nSong Popularity\nDuration (ms)\nSong Rank\nFollowers\nPopularity\nTotal Artists On Song\nLyrics Word Count\nneg\nneu\npos\ncompound\n\n\n\n\ncount\n94.000000\n94.000000\n94.000000\n9.400000e+01\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.00000\n\n\nmean\n63.510638\n212252.319149\n49.372340\n1.163717e+07\n72.627660\n1.319149\n233.702128\n0.092543\n0.767011\n0.140436\n0.29302\n\n\nstd\n19.227973\n57749.294460\n29.438467\n2.373635e+07\n17.228374\n0.882479\n179.263177\n0.071248\n0.107798\n0.077664\n0.79927\n\n\nmin\n17.000000\n120792.000000\n1.000000\n1.273000e+03\n29.000000\n1.000000\n0.000000\n0.000000\n0.454000\n0.000000\n-0.99980\n\n\n25%\n48.250000\n175601.000000\n24.250000\n2.208655e+05\n60.000000\n1.000000\n132.500000\n0.034250\n0.701250\n0.093750\n-0.68960\n\n\n50%\n66.000000\n196731.500000\n48.500000\n1.710836e+06\n77.000000\n1.000000\n169.000000\n0.084000\n0.757500\n0.131000\n0.71875\n\n\n75%\n77.750000\n241314.500000\n75.500000\n8.641217e+06\n86.750000\n1.000000\n258.750000\n0.140750\n0.851000\n0.189750\n0.96500\n\n\nmax\n98.000000\n456933.000000\n100.000000\n1.280014e+08\n100.000000\n8.000000\n1160.000000\n0.373000\n1.000000\n0.426000\n0.99840\n\n\n\n\n\n\n\n\n\nHistograms\nThe next step is checking the distributions of all of my numeric variables to see if there’s anything interesting. Keeping in mind that these are considered the top 100 songs of the week that we chose, I analyzed the characteristics of the songs and their lyrics.\n\n# Create individual histograms for selected columns\ncolumns_to_plot = ['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound']\nfor column in columns_to_plot:\n    plt.figure(figsize=(6, 4))\n    plt.hist(df_cl[column], edgecolor='black', alpha=0.7)\n    plt.title(f\"Histogram for {column}\")\n    plt.xlabel(column)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above graphs, it is interesting to observe that the popularity of the song is not as heavily skewed as I thought. Since these are the top songs, I expected the spotify popularity score to be generally very high, but this was not the case. Additionally, it is interesting to see so many trending songs from artists without very many followers on Spotify. I think this is indicative of the popularity that Tik Tok and other social media platforms can provide for less known artists. Despite this, the popularity of the artists on Spotify seems to be heavily skewed. Moving on to analysis about characteristics about the lyrics, it appears that the songs are largely not negative and tend to be more neutral or positive. This can be seen by the histogram for the compound score as well, with this graph showing a large number of songs with a positive compound score.\n\n\nGenre Analysis\nI decided to group the really diverse genres into distinct categories. I used chatgpt to create the categories and asked it to give preference to genres that are not pop if multiple genres are included. Then I mapped the genre column to my new categories and made a histogram.\n\ndef map_genres_to_categories(genres):\n    # Define the mapping of specific genres to broader categories\n    genre_mapping = {\n        \"Pop\": [\n            \"pop\", \"social media pop\", \"dance pop\", \"modern country pop\", \"gen z singer-songwriter\", \"colombian pop\", \n            \"modern indie pop\", \"power pop\", \"la pop\", \"singer-songwriter pop\", \"uk pop\", \"alt z\"\n        ],\n        \"Indie/Alternative\": [\n            \"indie pop\", \"modern alternative pop\", \"indie rock\", \"small room\", \"asheville indie\", \"sacramento indie\", \n            \"twee pop\", \"tape club\", \"bubblegrunge\", \"bedroom pop\", \"slacker rock\", \"irish indie rock\", \"irish post-punk\"\n        ],\n        \"Rock\": [\n            \"album rock\", \"blues rock\", \"classic rock\", \"electric blues\", \"hard rock\", \"jam band\", \"southern rock\", \n            \"heartland rock\", \"garage rock\", \"modern blues rock\", \"modern rock\", \"punk blues\", \"noise rock\", \"no wave\"\n        ],\n        \"Country\": [\n            \"classic texas country\", \"contemporary country\", \"country dawn\", \"country road\", \"countrygaze\", \n            \"deep new americana\", \"roots americana\", \"classic oklahoma country\", \"red dirt\"\n        ],\n        \"Hip Hop/Rap\": [\n            \"hip hop\", \"rap\", \"west coast rap\", \"pop rap\", \"melodic rap\", \"houston rap\", \"atl hip hop\", \"trap\", \n            \"trap queen\", \"viral rap\", \"southern hip hop\", \"dfw rap\", \"canadian hip hop\", \"indian underground rap\", \n            \"desi hip hop\", \"malayalam hip hop\", \"irish hip hop\", \"conscious hip hop\"\n        ],\n        \"R&B\": [\n            \"r&b\", \"alternative r&b\", \"uk contemporary r&b\", \"afro r&b\"\n        ],\n        \"Latin\": [\n            \"reggaeton\", \"urbano latino\", \"trap latino\", \"reggaeton chileno\", \"reggaeton colombiano\", \"latin pop\"\n        ],\n        \"Afrobeat/African\": [\n            \"afrobeats\", \"afropop\", \"azonto\", \"nigerian pop\", \"alte\", \"nigerian hip hop\"\n        ],\n        \"Electronic/Experimental\": [\n            \"art pop\", \"metropopolis\", \"ambient folk\", \"freak folk\", \"hyperpop\", \"proto-hyperpop\", \"bubblegum bass\", \n            \"digital hardcore\", \"escape room\", \"experimental pop\", \"experimental hip hop\", \"deconstructed club\", \n            \"electronica\", \"glitch\", \"glitch hop\", \"jazztronica\", \"intelligent dance music\", \"psychedelic hip hop\", \n            \"wonky\", \"indietronica\", \"afrofuturism\", \"transpop\", \"uk alternative pop\", \"crank wave\", \"jersey club\"\n        ],\n        \"K-pop\": [\n            \"k-pop\", \"k-pop girl group\", \"anime\"\n        ],\n        \"Other\": [\n            \"None Listed\"\n        ]\n    }\n\n    for category, genre_list in genre_mapping.items():\n        if genres.split(\", \")[0] in genre_list:\n            return category\n        elif len(genres.split(\",\"))&gt;1:\n            if genres.split(\", \")[1] in genre_list:\n                return category\n    return \"Other\"\n\ndf_cl['Simplified Genre'] = df_cl['Genres'].apply(map_genres_to_categories)\ndf_cl[['Genres','Simplified Genre']].head()\n\n\n\n\n\n\n\n\nGenres\nSimplified Genre\n\n\n\n\n0\nindie pop\nIndie/Alternative\n\n\n1\nconscious hip hop, hip hop, rap, west coast rap\nHip Hop/Rap\n\n\n2\nmodern country pop, pop rap\nPop\n\n\n3\npop\nPop\n\n\n4\nart pop, pop\nPop\n\n\n\n\n\n\n\n\nplt.hist(df_cl['Simplified Genre'], edgecolor='black', alpha=0.7)\nplt.title(f\"Histogram for Simplified Genre\")\nplt.xlabel(\"Simplified Genre\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nFrom this, we can see that pop clearly dominates the top of the charts, but there is still a lot of diversity in the genre.\n\nplt.hist(df_cl['Explicit'].astype(int), edgecolor='black', alpha=0.7)\nplt.title(f\"Histogram for Explicit Songs\")\nplt.xlabel(\"Explicit\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nMost songs are not explicit but some of them are.\n\n\n\nBivariate and Multivariate Analysis\n\n# Create a new column with bins of size 10\ndf_cl[\"Ranking Group\"] = pd.cut(\n    df_cl[\"Song Rank\"], \n    bins=range(0, 101, 10),\n    labels=[f\"{i+1}-{i+10}\" for i in range(0, 100, 10)],\n    right=True\n)\ndf_cl[\"Top 10 Song\"] = df_cl[\"Song Rank\"].apply(lambda x: 1 if x &lt;= 10 else 0)\ndf_cl[[\"Ranking Group\", \"Song Rank\", \"Top 10 Song\"]]\n\n\n\n\n\n\n\n\nRanking Group\nSong Rank\nTop 10 Song\n\n\n\n\n0\n1-10\n1\n1\n\n\n1\n1-10\n2\n1\n\n\n2\n1-10\n3\n1\n\n\n3\n1-10\n4\n1\n\n\n4\n1-10\n5\n1\n\n\n...\n...\n...\n...\n\n\n89\n91-100\n96\n0\n\n\n90\n91-100\n97\n0\n\n\n91\n91-100\n98\n0\n\n\n92\n91-100\n99\n0\n\n\n93\n91-100\n100\n0\n\n\n\n\n94 rows × 3 columns\n\n\n\n\nsns.pairplot(df_cl[['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound', 'Ranking Group']], hue=\"Ranking Group\")\n\n\n\n\n\n\n\n\nThe above graph is a pairplot of the numeric columns and the hue is the ranking groups that I made to bin ranking by groups of 10. I’m not seeing any clear clusters appearing here between any of the variables. There is a positive linear relationship between artist popularity and song popularity though. This might be expected due to the way that spotify calculates this popularity and this high correlation might mean we should remove one of these variables when creating models.\n\nsns.pairplot(df_cl[['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound', 'Top 10 Song']], hue=\"Top 10 Song\")\n\n\n\n\n\n\n\n\nThis is a pairplot where only the top 10 songs are colored. There aren’t many clear distinctions but I am curious about the clustering in the scatter plot between song popularity and song duration as well as song popularity and positive sentiment score.\n\ncorr = df_cl[['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', \n              'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound']].corr()\n\n# Plot the correlation matrix\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nThis correlation plot confirms my earlier observations about popularity and song popularity being highly correlated. It also shows the relationships between the various outputs of VADER which is to be expected and might require that we only use one of the 4 output scores during analysis. There is also the obvious correlation between lyrics word count and duration, although I expected this to be much higher. I think the slight correlation between negative lyric sentiment and lyric word count is interesting. There is also a slight negative correlation between positive lyric sentiment and song duration.\n\ngenre_counts = df_cl.groupby(['Ranking Group', 'Simplified Genre'], observed=True).size().unstack()\ngenre_counts.plot(kind='bar', stacked=True, figsize=(10,6))\nplt.title(\"Genre Distribution by Ranking Group\")\nplt.xlabel(\"Ranking Group\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nThis is a breakdown of the genres by ranking group. The distribution of genres seems to be unchanged across the different ranking groups (as in there is a mix of genres across each group). However, some genres don’t appear at all in the top 20, including K-Pop and R&B.\n\nsns.boxplot(data=df_cl, x='Ranking Group', y='Lyrics Word Count')\nplt.title(\"Lyrics Word Count by Ranking Group\")\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nThis is a breakdown of the word count by group, with the highest and lowest word count groups not being in the top 10.\n\nsns.boxplot(data=df_cl, x='Ranking Group', y='compound')\nplt.title(\"Compound by Ranking Group\")\nplt.show()\n\n\n\n\n\n\n\n\nThis is a graph of the compound score by ranking group. Overall, the median seems to increase as the rankings increase, possibly indicating that songs that are ranked higher are less positive (or even negative).\n\nsns.scatterplot(x='Duration (ms)', y='Song Popularity', hue='Top 10 Song', data=df_cl)\nplt.title(\"Song Popularity vs. Duration (ms) by Top 10 Song\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows the Duration of the song vs it’s popularity. It seems that the most popular songs are between 150000 and 275000 ms, with most of the top 10 songs falling between 175000 and 225000.\n\nsns.scatterplot(x='Lyrics Word Count', y='Popularity', hue='Top 10 Song', data=df_cl)\nplt.title(\"Popularity vs. Lyrics Word Count by Ranking Group\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows the word count of the song vs it’s popularity. Most of the top 10 songs are concentrated between 150 and 300, with only 2 outliers for this.\n\n# Create crosstab for Ranking Group vs Explicit\nexplicit_ranking_crosstab = pd.crosstab(df_cl['Ranking Group'], df_cl['Explicit'])\n\n# Plot a grouped bar plot\nexplicit_ranking_crosstab.plot(kind='bar', stacked=False)\nplt.title('Ranking Group vs Explicit')\nplt.xlabel('Ranking Group')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.legend(title='Explicit')\nplt.show()\n\n\n\n\n\n\n\n\nThis graph shows the distribution of explicit songs in each category. For the middle categories, it is interesting that there is a perfect split between explicit and not explicit songs. The top category has far more non explicit songs compared to explicit songs. This is also true of the bottom categories, however, so this might not be the most predictive feature.\n\n# Create crosstab for Top 10 Song vs Simplified Genre\ntop10_genre_crosstab = pd.crosstab(df_cl['Top 10 Song'], df_cl['Simplified Genre'])\n\n# Plot a grouped bar plot\ntop10_genre_crosstab.plot(kind='bar', stacked=False)\nplt.title('Top 10 Song vs Simplified Genre')\nplt.xlabel('Top 10 Song')\nplt.ylabel('Count')\nplt.legend(title='Simplified Genre')\nplt.show()\n\n\n\n\n\n\n\n\nThis is another look at the distribution of genres between top 19 songs and not top 10 songs. There are a lot of cateogries missing from the top 10 and the heaviest weight is on pop and indie, which could be a good predictor of the top songs.\n\n\nData Distribution and Normalization\n\ncontinuous_columns = ['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound']\n\n# Create histograms and density plots for each continuous column\nplt.figure(figsize=(15, 10))\n\nfor i, column in enumerate(continuous_columns, 1):\n    plt.subplot(3, 3, i)  # 3 rows, 4 columns\n    sns.histplot(df_cl[column], kde=True, bins=30, color='skyblue')\n    plt.title(f'Distribution of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis is a figure for a histogram and density plot for each of the continuous variables of interest.\n\nSong popularity: roughly bell shaped, some skewing, most songs between 40-90 - relatively evenly distributed with a more songs having a higher popularity\nDuration: high right skew, most songs are shorter (2.5-3 minutes) with very few songs having longer durations\nFollowers: highly skewed, few artists have a large number of followers while most have low follower counts\nArtist Popularity: rough bell curve, somewhat skewed, concentrated in 70-90 range, most songs come from popular artists\nWord Count: most have fewer than 200 words, right skew\nNegative sentiment: Relatively even spread from 0-0.3, most have low neg sentiment\nNetural sentiment: most between 0.7-0.9, pretty even, most have medium to high neutral\nPositive sentimet: slightly bell shaped, right skew, scores between 0.05-0.15, pretty low pos sentiment overall\nCompound: mis of pos and neg values but most are closer to 0 than extremes (except for positive 1)\n\nMost of the variables are skewed. Some variable might be normal-like but are still skewed.\n\nskewness = df_cl[continuous_columns].apply(skew)\nkurt = df_cl[continuous_columns].apply(kurtosis)\n\nprint(\"Skewness:\")\nprint(skewness)\nprint(\"\\nKurtosis:\")\nprint(kurt)\n\nSkewness:\nSong Popularity     -0.377718\nDuration (ms)        1.665246\nFollowers            3.097444\nPopularity          -0.588679\nLyrics Word Count    2.480521\nneg                  1.076954\nneu                 -0.235075\npos                  0.610432\ncompound            -0.647878\ndtype: float64\n\nKurtosis:\nSong Popularity     -0.671813\nDuration (ms)        4.075121\nFollowers            9.910030\nPopularity          -0.565867\nLyrics Word Count    7.537863\nneg                  1.705756\nneu                  0.266109\npos                  1.037166\ncompound            -1.363075\ndtype: float64\n\n\nThe skewness and kurtosis test results confirm the above findings. Followers, Duration, and Lyrics Word Count all have a highly positive kurtosis, indicating a lot of outliers. I will apply a log transformation to these variables to try to decrease this skewness.\n\n# Apply log transformation to variables with high skewness\ndf_cl['Duration (ms)_transf'] = np.log1p(df_cl['Duration (ms)'])\ndf_cl['Followers_transf'] = np.log1p(df_cl['Followers'])\ndf_cl['Lyrics Word Count_transf'] = np.log1p(df_cl['Lyrics Word Count'])\n\n# Check the transformations\ndf_cl[['Duration (ms)_transf', 'Followers_transf', 'Lyrics Word Count_transf']].describe()\n\n\n\n\n\n\n\n\nDuration (ms)_transf\nFollowers_transf\nLyrics Word Count_transf\n\n\n\n\ncount\n94.000000\n94.000000\n94.000000\n\n\nmean\n12.234224\n14.219630\n5.231510\n\n\nstd\n0.244274\n2.462787\n0.776593\n\n\nmin\n11.701834\n7.149917\n0.000000\n\n\n25%\n12.075974\n12.305041\n4.893915\n\n\n50%\n12.189590\n14.351124\n5.135781\n\n\n75%\n12.393800\n15.971873\n5.559481\n\n\nmax\n13.032294\n18.667552\n7.057037\n\n\n\n\n\n\n\n\n# Create histograms and density plots for each continuous column\nplt.figure(figsize=(15, 10))\n\nfor i, column in enumerate(['Duration (ms)_transf', 'Followers_transf', 'Lyrics Word Count_transf'], 1):\n    plt.subplot(3, 3, i)  # 3 rows, 4 columns\n    sns.histplot(df_cl[column], kde=True, bins=30)\n    plt.title(f'Distribution of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI decided to apply a log transformation to Duration, Followers, and Word Count so that their distributions would be much less skewed. After looking at the histograms and density plots, I can see that this in fact helped the skewness and helped make the data more normal. Doing this will allow for clustering models to fit better on the data and make for a more clear insights about the data. It also decreases the outliers that’ll impact the model. The next step is rescaling the data to make the range of data the same across all of the features.\n\n# Turn percentages back into decimals\ndf_cl['Popularity'] = df_cl['Popularity']/100\ndf_cl['Song Popularity'] = df_cl['Song Popularity']/100\n\n# Selecting columns that were transformed\ncolumns_to_normalize = ['Duration (ms)_transf', 'Followers_transf', 'Lyrics Word Count_transf']\n\n# Initialize scalers\nmin_max_scaler = MinMaxScaler()\n\n# Apply Min-Max Scaling\ndf_cl_minmax = df_cl[columns_to_normalize].copy()\ndf_cl_minmax[columns_to_normalize] = min_max_scaler.fit_transform(df_cl_minmax[columns_to_normalize])\n\n# Plotting the results\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Original distributions\nfor i, column in enumerate(columns_to_normalize):\n    sns.histplot(df_cl[column], kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f\"Original Distribution of {column}\")\n    \n# Normalized distributions\nfor i, column in enumerate(columns_to_normalize):\n    sns.histplot(df_cl_minmax[column], kde=True, ax=axes[1, i])\n    axes[1, i].set_title(f\"Min-Max Scaled Distribution of {column}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_cl[columns_to_normalize] = df_cl_minmax[columns_to_normalize]\ndf_cl.describe()\n\n\n\n\n\n\n\n\nSong Popularity\nDuration (ms)\nSong Rank\nFollowers\nPopularity\nTotal Artists On Song\nLyrics Word Count\nneg\nneu\npos\ncompound\nTop 10 Song\nDuration (ms)_transf\nFollowers_transf\nLyrics Word Count_transf\n\n\n\n\ncount\n94.000000\n94.000000\n94.000000\n9.400000e+01\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.00000\n94.000000\n94.000000\n94.000000\n94.000000\n\n\nmean\n0.635106\n212252.319149\n49.372340\n1.163717e+07\n0.726277\n1.319149\n233.702128\n0.092543\n0.767011\n0.140436\n0.29302\n0.106383\n0.400155\n0.613816\n0.741318\n\n\nstd\n0.192280\n57749.294460\n29.438467\n2.373635e+07\n0.172284\n0.882479\n179.263177\n0.071248\n0.107798\n0.077664\n0.79927\n0.309980\n0.183601\n0.213827\n0.110045\n\n\nmin\n0.170000\n120792.000000\n1.000000\n1.273000e+03\n0.290000\n1.000000\n0.000000\n0.000000\n0.454000\n0.000000\n-0.99980\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.482500\n175601.000000\n24.250000\n2.208655e+05\n0.600000\n1.000000\n132.500000\n0.034250\n0.701250\n0.093750\n-0.68960\n0.000000\n0.281211\n0.447585\n0.693480\n\n\n50%\n0.660000\n196731.500000\n48.500000\n1.710836e+06\n0.770000\n1.000000\n169.000000\n0.084000\n0.757500\n0.131000\n0.71875\n0.000000\n0.366607\n0.625233\n0.727753\n\n\n75%\n0.777500\n241314.500000\n75.500000\n8.641217e+06\n0.867500\n1.000000\n258.750000\n0.140750\n0.851000\n0.189750\n0.96500\n0.000000\n0.520095\n0.765952\n0.787792\n\n\nmax\n0.980000\n456933.000000\n100.000000\n1.280014e+08\n1.000000\n8.000000\n1160.000000\n0.373000\n1.000000\n0.426000\n0.99840\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nUsing min-max scaling on these 3 columns changes them from a scale on the hundreds of thousands to a scale from 0 to 1. This will make sure that these features don’t disproportionately have a bigger influence on a model. It also puts these features on an equal scale as all of the other features which will be very beneficial when running clustering algorithms.\n\ndf_cl.to_csv('../../data/processed-data/transformed_data.csv', index=False)\n\n\n\nStatistical Insights\nI want to run a t-test on the two groups (top 10 songs vs not) to see if there is a difference in all of the different variables between the two groups. I decided to write a function that accomplishes this and then prints out the correct conclusion based on the resulting p-value.\n\ndef t_test_top_10(df, test_col):\n    top_10 = df[df['Top 10 Song'] == 1][test_col]\n    not_top_10 = df[df['Top 10 Song'] == 0][test_col]\n    t_stat, p_val = ttest_ind(top_10, not_top_10, equal_var=False)\n\n    if p_val &lt; 0.05:\n        print(\"Test on \" + test_col)\n        print(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4f}\")\n        print(f\"We can reject the null hypothesis and conclude that the difference in {test_col} between Top 10 Songs and others is statistically significant\")\n    # else:\n    #     print(f\"We fail to reject the null hypothesis and can't conclude that there is a statistically significant difference in {test_col} between Top 10 Songs and others\")\n\n\npossible_columns = ['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound', 'Duration (ms)_transf', 'Followers_transf', 'Lyrics Word Count_transf']\n\nfor col in possible_columns:\n    t_test_top_10(df_cl, col)\n\nTest on Song Popularity\nT-statistic: 4.6804, P-value: 0.0003\nWe can reject the null hypothesis and conclude that the difference in Song Popularity between Top 10 Songs and others is statistically significant\nTest on Popularity\nT-statistic: 3.2110, P-value: 0.0064\nWe can reject the null hypothesis and conclude that the difference in Popularity between Top 10 Songs and others is statistically significant\nTest on Followers_transf\nT-statistic: 2.4122, P-value: 0.0303\nWe can reject the null hypothesis and conclude that the difference in Followers_transf between Top 10 Songs and others is statistically significant\n\n\nThe variables that we can reject the null hypothesis include: Song Popularity, Artist Popularity, and Artist Followers. I think this means that we can say that the more popular an artist is, the more likely they are to have a song in the top 10. This is a very significant conclusion in terms of what it means for new artists trying to break into the industry.\nNext, I wanted to run a 1 way Anova test on if there is a difference between the different ranking groups and the possible variables. Again, I printed out the results of the test pertaining to the hypotheses to see what I could conclude.\n\ndef one_way_ANOVA(df, test_col):\n    group_1 = df[df['Ranking Group'] == '1-10'][test_col]\n    group_2 = df[df['Ranking Group'] == '11-20'][test_col]\n    group_3 = df[df['Ranking Group'] == '21-30'][test_col]\n    group_4 = df[df['Ranking Group'] == '31-40'][test_col]\n\n    # Perform ANOVA\n    f_stat, p_val = f_oneway(group_1, group_2, group_3, group_4)\n\n    # Output results\n    if p_val &lt; 0.05:\n        print(\"Running test on \" + test_col)\n        print(f\"F-statistic: {f_stat:.4f}, P-value: {p_val:.4f}\")\n        print(f\"There are significant differences in {test_col} across Ranking Groups.\")\n\n\nfor col in possible_columns:\n    one_way_ANOVA(df_cl, col)\n\nRunning test on Song Popularity\nF-statistic: 2.9815, P-value: 0.0441\nThere are significant differences in Song Popularity across Ranking Groups.\n\n\nThe only test that resulted in a significant difference between ranking groups is the Song Popularity, which makes sense because songs that are ranked more popular are ranked so because they have achieved success."
  },
  {
    "objectID": "technical-details/eda/main.html#conclusions-and-next-steps",
    "href": "technical-details/eda/main.html#conclusions-and-next-steps",
    "title": "Exploratory Data Analysis",
    "section": "Conclusions and Next Steps",
    "text": "Conclusions and Next Steps\n\nSummary of EDA Findings\nFrom the exploratory data analysis, several important trends and patterns were identified:\n\nUnivariate Analysis\n\nGenre Distribution: Pop dominates the top 100 songs, but there is still a variety of other genres represented.\nCompound Sentiment Scores: Most songs exhibit very positive compound sentiment scores, with few songs having neutral scores. This suggests that emotional songs (either highly positive or highly negative) dominate.\nArtist Representation: There is a good presence of less popular artists achieving hits, suggesting opportunities for newer or niche artists.\nExplicit Content: While most songs are not explicit, a significant proportion still fall under the explicit category.\n\n\n\nBivariate Analysis\n\nArtist and Song Popularity: There is a strong positive linear relationship between artist popularity and song popularity, likely due to Spotify’s calculation methods.\nVADER Sentiment Scores: High correlation exists between the four VADER sentiment scores (positive, negative, neutral, compound) as expected.\nTop 10 Songs: These songs cluster together in terms of song popularity versus duration and song popularity versus positive score, indicating potential shared characteristics of highly successful songs.\nNegative Sentiment and Song Length: Slight positive correlation between negative sentiment scores and lyric word count suggests that longer songs tend to have more negative sentiment.\nPositive Sentiment and Song Length: A slight negative correlation between positive sentiment scores and song duration shows that shorter songs tend to have a more positive tone.\nLyric Word Count and Duration: There is an obvious correlation between word count and song duration, though it is not as strong as initially expected.\n\n\n\nRankings\n\nGenre Distribution by Rank: All ranking bins show a wide variety of genres, but some (e.g., K-Pop, R&B) are not in the top 20 songs.\nSentiment and Rank: The median compound sentiment score increases with rank, indicating that more positively emotional songs tend to achieve higher rankings.\n\n\n\nTransformations\n\nSeveral variables, such as duration, artist followers, and lyric word count, exhibited skewness. Log transformation and min-max scaling successfully reduced these skews, preparing the data for further analysis.\n\n\n\nStatistical Tests\n\nVariables such as song popularity, artist popularity, and artist followers showed statistically significant differences between the top 10 songs and other songs (via t-tests). This suggests that more popular artists are more likely to have a song in the top 10, which has major implications for emerging artists attempting to achieve similar success.\n\n\n\n\nImplications for Modeling\n\nFeature Selection:\n\nKey variables such as song popularity, artist popularity, compound sentiment scores, and explicit content should be included as potential predictors in the modeling process.\nVariables like lyric word count and duration could also be significant based on their correlations with sentiment and popularity.\n\nData Transformation:\n\nSkewed variables like duration, followers, and word count should use the log-transformed versions to improve model performance.\nMin-max scaling is also recommended for these variables to ensure comparability in models sensitive to feature scaling.\n\n\nThe EDA successfully allowed us to explore the different features that we collected from various sources. With these insights and transformations, we can move forward to make models and better understand the underlying dynamics of this data."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is a critical step in ensuring the quality and usability of data for analysis. In this project, the data cleaning process involved merging data frames, creating more-indepth columns, handling missing variables, and coverting data types.\nSince we want to analyze artists, songs, and lyrics, we conducted several changes such as adding song ranking, one hot encoding the genres, conducting sentiment analysis on the lyrics, and looking into artists features in a song. These will help streamline the EDA process as well as our analysis."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-to-data-cleaning",
    "href": "technical-details/data-cleaning/main.html#introduction-to-data-cleaning",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is a critical step in ensuring the quality and usability of data for analysis. In this project, the data cleaning process involved merging data frames, creating more-indepth columns, handling missing variables, and coverting data types.\nSince we want to analyze artists, songs, and lyrics, we conducted several changes such as adding song ranking, one hot encoding the genres, conducting sentiment analysis on the lyrics, and looking into artists features in a song. These will help streamline the EDA process as well as our analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "Spotify API: Used to collect information about songs and artists.\n\nSpotify API Documentation\n\nGenius API: Used to retrieve song lyrics.\n\nGenius API Documentation\n\n\nSpotify API provides detailed metada about tracks and artists, which is important as we want to look at the song details, artist details, and popularity. Genius API provies lyrics which can be used for word analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-source-information",
    "href": "technical-details/data-collection/main.html#data-source-information",
    "title": "Data Collection",
    "section": "",
    "text": "Spotify API: Used to collect information about songs and artists.\n\nSpotify API Documentation\n\nGenius API: Used to retrieve song lyrics.\n\nGenius API Documentation\n\n\nSpotify API provides detailed metada about tracks and artists, which is important as we want to look at the song details, artist details, and popularity. Genius API provies lyrics which can be used for word analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-method",
    "href": "technical-details/data-collection/main.html#data-collection-method",
    "title": "Data Collection",
    "section": "Data Collection Method",
    "text": "Data Collection Method\n\nSpotify API\n\nUsed spotipy library to authenticate client ID and client secret.\nAfter pulling the songs from Rolling Stone’s spotify Top 100 playlist, we pullled all the information of the artists and the tracks into seperate csvs.\n\nGenius API\n\nAuthenticated using client access token.\nGot the lyrics url for each song by querying Genius for song details.\nUsed BeautifulSoup to scrape lyrics from the url.\n\nBeautifulSoup is a Python library used for web scraping. It allows us to get data from HTML and XML files."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-structure-and-format",
    "href": "technical-details/data-collection/main.html#data-structure-and-format",
    "title": "Data Collection",
    "section": "Data Structure and Format",
    "text": "Data Structure and Format\n\nartists_data.csv\n\nRows represent individual artists with columns: Artist ID, Name, Genres, Followers, Popularity.\n\nrolling_stone_top_100,csv\n\nRows represent each song with columns: Track Name, Artists.\n\nsong_lyrics.csv\n\nRows represent each song with columns: Track Name, Artists, lyrics.\n\ntracks_data.csv\n\nRows represent each song with columns: Track ID, Track Name, Popularity, Album, Release Date, Duration (ms), Artist, Explicit."
  },
  {
    "objectID": "technical-details/data-collection/main.html#linking-to-data",
    "href": "technical-details/data-collection/main.html#linking-to-data",
    "title": "Data Collection",
    "section": "Linking to Data",
    "text": "Linking to Data\n\nartists_data.csv\nrolling_stone_top_100,csv\nsongs_lyrics.csv\ntracks_data.csv"
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "Goals",
    "text": "Goals\nThe primary goal of this notebook is to colelct, process and analyze song metadata and lyrics from multiple APIS (Spotify and Genius) to analyze song popularity and its predictors. We can acheive this by acuiring data from songs that are already considered the top 100 of 2024, cleaning the dataset, conduting exploratory data analysis, machine learning analysis, and predictive analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "Motivation",
    "text": "Motivation\nThis work is motivated by the need to bridge the gap between song qualities, artist features, and their impact on song popularity. By combining metadata with lyrical analysis, we aim to illustrate patterns that can better enhance music recommendations, audience targeting strategies, and creative decision-making for artists."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "Objectives",
    "text": "Objectives\n\nRetrieve song and artist details using the Spotify API.\nExtract lyrics using the Genius API and BeautifulSoup.\nNormalize and clean the collected data.\nStructure datasets for exploratory data analysis (EDA) and machine learning workflows.\nIdentify key features."
  },
  {
    "objectID": "technical-details/data-collection/main.html#methods",
    "href": "technical-details/data-collection/main.html#methods",
    "title": "Data Collection",
    "section": "Methods",
    "text": "Methods\nThis project utilizes a combination of API, web scraping, and data processing techniques to collect and prepare song metadata and lyrics for analysis.\n\nData Collection: Song metadata was retrieved using the Spotify API with spotipy, while lyrics were obtained through the Genius API and scraped with BeautifulSoup.\nData Processing: Data was cleaned, normalized, and merged.\nTools Used: Libraries such as spotipy, requests, BeautifulSoup, and pandas were employed for API interaction, HTML parsing, and data manipulation.\n\nThese methods provides a foundation for preparing the data for analysis and modeling tasks."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nData Retrieval:\n\nSpotify API: Spotify API does not let you handle more than 50 requests.\nGenius API: Inconsistencies in matching song titles and artists between Spotify and Genius.\n\nWeb Scraping:\n\nPage structures on Genius required fine-tuning of BeautifulSoup parsing logic to extract lyrics.\nSome songs were purely instrumental so lyrics are missing.\n\nData Integration:\n\nDifferences in naming conventions and formats across APIs required additional care.\nSince some songs have artists features, the amount of artists extracted are more than the amount of songs extracted."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nSpotify’s popularity scores align with expected trends, serving as a basis for analysis.\nSpotify metadata and Genius lyrics offers a strong foundation for future analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion",
    "href": "technical-details/data-collection/main.html#conclusion",
    "title": "Data Collection",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nThe project successfully gathered song metadata and lyrics.\nThe resulting datasets are comprehensive and ready for exploratory data analysis, clustering, and predictive modeling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#future-steps",
    "href": "technical-details/data-collection/main.html#future-steps",
    "title": "Data Collection",
    "section": "Future Steps:",
    "text": "Future Steps:\n\nDevelop solutions for handling mismatched data between Spotify and Genius.\nInclude additional playlists or genres for in-depth analysis.\nConduct sentiment analysis on song lyrics and compare results with Spotify’s popularity scores.\nConduct machine learning models to identify patterns and predictors of song popularity."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this supervised analysis, we aim to evaluate features that contribute the most to song popularity. This is going to be done through different types of machine learning models. By selecting the most relevant features, we can improve model accuracy."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this supervised analysis, we aim to evaluate features that contribute the most to song popularity. This is going to be done through different types of machine learning models. By selecting the most relevant features, we can improve model accuracy."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods",
    "title": "Supervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nTo predict song popularity, the following regression models were implemented: - Linear Regression - Random Forest Method - Gradient Boosting Regression"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#interpretation",
    "href": "technical-details/supervised-learning/main.html#interpretation",
    "title": "Supervised Learning",
    "section": "Interpretation",
    "text": "Interpretation\nIt seems like ‘Followers’ is the most important aspect to sconsider for song popularity. This means, even if the song is really good, the artists should promote themselves.\n\n#Figure shows the actual vs predict popularity scores \nplt.scatter(y_test, y_rf_pred, alpha=0.6, color='blue', label='Predictions')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Perfect Fit')  # Line of perfect predictions\nplt.title(\"Actual vs Predicted Popularity Scores\")\nplt.xlabel(\"Actual Popularity Score\")\nplt.ylabel(\"Predicted Popularity Score\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX.dropna(inplace=True)\n#There are two targets \ny_multi = masterlist_data[['Song Popularity', 'Song Rank']]\ny_multi = y_multi.loc[X.index]\n\n#Split in 80/20\nX_train, X_test, y_train, y_test = train_test_split(X, y_multi, test_size=0.2, random_state=42)\n\n#Using GradientBoostingRegressor\nmulti_regressor = MultiOutputRegressor(GradientBoostingRegressor(random_state=42))\nmulti_regressor.fit(X_train, y_train)\n\ny_multi_pred = multi_regressor.predict(X_test)\n\nmse_popularity = mean_squared_error(y_test['Song Popularity'], y_multi_pred[:, 0])\nmse_rank = mean_squared_error(y_test['Song Rank'], y_multi_pred[:, 1])\n\nprint(f\"Popularity MSE: {mse_popularity}\")\nprint(f\"Rank MSE: {mse_rank}\")\n\n/var/folders/c4/ckrgfkld6ll_8d5glczkzftw0000gn/T/ipykernel_16209/4181636287.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X.dropna(inplace=True)\n\n\nPopularity MSE: 210.31359827795765\nRank MSE: 1.038648227890669\n\n\n\n#Splitting the output by song popularity and song rank so we can individually graph them \nactual_popularity = y_test['Song Popularity']\npredicted_popularity = y_multi_pred[:, 0]\nactual_rank = y_test['Song Rank']\npredicted_rank = y_multi_pred[:, 1]\n\n#This is for song popularity\nplt.scatter(actual_popularity, predicted_popularity, color='blue', label='Predictions')\nplt.plot([actual_popularity.min(), actual_popularity.max()], \n         [actual_popularity.min(), actual_popularity.max()], \n         color='red', linestyle='--', label='Perfect Fit')\nplt.title('Actual vs Predicted Song Popularity')\nplt.xlabel('Actual Popularity')\nplt.ylabel('Predicted Popularity')\nplt.legend()\nplt.show()\n\n#This is for song rank \nplt.scatter(actual_rank, predicted_rank, color='green', label='Predictions')\nplt.plot([actual_rank.min(), actual_rank.max()], \n         [actual_rank.min(), actual_rank.max()], \n         color='red', linestyle='--', label='Perfect Fit')\nplt.title('Actual vs Predicted Song Rank')\nplt.xlabel('Actual Rank')\nplt.ylabel('Predicted Rank')\nplt.legend()\n\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#interpretation-1",
    "href": "technical-details/supervised-learning/main.html#interpretation-1",
    "title": "Supervised Learning",
    "section": "Interpretation",
    "text": "Interpretation\nThere seems to be an overfitting with song rank, but the MSE score shows that there is a good predictors in song rank while a low prediction in song popularity.\n\n#Analysing song popularity based on sentiment outputs \nX = masterlist_data[['Sentiment_Pos', 'Sentiment_Neg', 'Sentiment_Neu', 'Explicit']]\ny = masterlist_data['Song Popularity']\n\n#Using test train split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Using Random Forest to train and test data \nrf_model = RandomForestRegressor(random_state=42)\nrf_model.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-Squared (R2): {r2}\")\n\nMean Squared Error (MSE): 574.7747460666667\nR-Squared (R2): 0.019724483974030815\n\n\n\n#Graphing the sentiment values with feature importance to see if positive, negative or neutral is better along with being explicit\nfeature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n\nfeature_importances.sort_values(ascending=False).plot(kind='bar', color='skyblue')\nplt.title('Feature Importance By Sentiment')\nplt.ylabel('Importance Score')\nplt.xlabel('Feature')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#interpretation-2",
    "href": "technical-details/supervised-learning/main.html#interpretation-2",
    "title": "Supervised Learning",
    "section": "Interpretation",
    "text": "Interpretation\nSentiment_Neg has the highest importance score. This shows that songs that have negative sentiment tend to have higher song popularity. A song being explicit or not does not seem to matter as much, which is surprising because we wonder if that could be due to song being played in public."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\n\n\n\nDefine the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\n\n\n\nPresent the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\n\n\n\nUse clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\n\n\n\nExplain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\n\n\n\nProvide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\n\n\n\nSummarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\n\n\n\n(Optional): Additional charts or explanations for further insights.\n\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#executive-summary",
    "href": "report/report.html#executive-summary",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”"
  },
  {
    "objectID": "report/report.html#objective",
    "href": "report/report.html#objective",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”"
  },
  {
    "objectID": "report/report.html#key-insights",
    "href": "report/report.html#key-insights",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”"
  },
  {
    "objectID": "report/report.html#visualizations",
    "href": "report/report.html#visualizations",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement."
  },
  {
    "objectID": "report/report.html#business-implications",
    "href": "report/report.html#business-implications",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”"
  },
  {
    "objectID": "report/report.html#recommendations",
    "href": "report/report.html#recommendations",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”"
  },
  {
    "objectID": "report/report.html#conclusion",
    "href": "report/report.html#conclusion",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”"
  },
  {
    "objectID": "report/report.html#appendix",
    "href": "report/report.html#appendix",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "(Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "LLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nBrainstorm ideas for further quantitative columns we could make from the data after finding out the limitations of the Spotify API\nHelp coming up with catchy title for project\nIdeas for multivariate eda graph types given the variables that I wanted to compare"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nHelp understand the Genius API and understand how to get a key to search the API\nHelp write webscraping function for Genius Lyric API\nExplanation of github usage through branches and merging\nGrouping genres into simplified groups for analysis\nAsked for information about pd crosstab and how to use it in eda\nProvided results from skewness and kurtosis and asked for suggestion on transformations that would be appropriate for the dataset."
  }
]