[
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "LLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nBrainstorm ideas for further quantitative columns we could make from the data after finding out the limitations of the Spotify API\nHelp coming up with catchy title for project\nIdeas for multivariate eda graph types given the variables that I wanted to compare"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nHelp understand the Genius API and understand how to get a key to search the API\nHelp write webscraping function for Genius Lyric API\nExplanation of github usage through branches and merging\nGrouping genres into simplified groups for analysis\nAsked for information about pd crosstab and how to use it in eda\nProvided results from skewness and kurtosis and asked for suggestion on transformations that would be appropriate for the dataset."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This is the page for exploratory data analysis. After making our processed data files, we wanted to explore the distributions of various features, the correlation between aspects of songs, and the statistical significance of differences between different categories. This will help us understand the data better, look for any preliminary interesting patterns and insights, and make informed decisions about the models we build as we go forward. Specifically, we looked at the song lyrics and the metadata about the music, their relationships, and their contribution onn song popularity and chart rankings. It will also allow us to look for anomalies to make sure that our data is suitable for ML tasks."
  },
  {
    "objectID": "technical-details/eda/main.html#introduction-and-motivation",
    "href": "technical-details/eda/main.html#introduction-and-motivation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This is the page for exploratory data analysis. After making our processed data files, we wanted to explore the distributions of various features, the correlation between aspects of songs, and the statistical significance of differences between different categories. This will help us understand the data better, look for any preliminary interesting patterns and insights, and make informed decisions about the models we build as we go forward. Specifically, we looked at the song lyrics and the metadata about the music, their relationships, and their contribution onn song popularity and chart rankings. It will also allow us to look for anomalies to make sure that our data is suitable for ML tasks."
  },
  {
    "objectID": "technical-details/eda/main.html#overview-of-methods",
    "href": "technical-details/eda/main.html#overview-of-methods",
    "title": "Exploratory Data Analysis",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nThis section uses data visualization, aggregation techniques, and conducts statistical tests to do a preliminary analysis of the data."
  },
  {
    "objectID": "technical-details/eda/main.html#code",
    "href": "technical-details/eda/main.html#code",
    "title": "Exploratory Data Analysis",
    "section": "Code",
    "text": "Code\n\nStep 1: Import data and clean data\n\nImported the data using pandas\nIsolated the rows that I thought might be helpful to analyze for this section of the analysis\nReplaced the null values in genre with “None listed” so I can still use the rest of the data (4 rows)\nChange VADER results into indvidual columns\nDisplayed the results\n\n\nimport pandas as pd\nimport ast\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('../../data/processed-data/artist_song_masterlist.csv')\ndf.head()\n\n\n\n\n\n\n\n\nTrack ID\nTrack Name\nSong Popularity\nAlbum\nSong Release Date\nDuration (ms)\nArtists\nExplicit\nSong Rank\nArtist ID\n...\nGenre_trap latino\nGenre_trap queen\nGenre_twee pop\nGenre_uk alternative pop\nGenre_uk contemporary r&b\nGenre_uk pop\nGenre_urbano latino\nGenre_viral rap\nGenre_west coast rap\nGenre_wonky\n\n\n\n\n0\n0WbMK4wrZ1wFSty9F7FCgu\nGood Luck, Babe!\n93.0\nGood Luck, Babe!\n2024-04-05\n218423.0\nChappell Roan\nFalse\n1\n7GlBOeep6PqTfFi59PTUUN\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n6AI3ezQ4o3HUoP6Dhudph3\nNot Like Us\n88.0\nNot Like Us\n2024-05-04\n274192.0\nKendrick Lamar\nTrue\n2\n2YZyLoL8N0Wb9xBt1NhZWg\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n2FQrifJ1N335Ljm3TjTVVf\nA Bar Song (Tipsy)\n86.0\nA Bar Song (Tipsy)\n2024-04-12\n171291.0\nShaboozey\nTrue\n3\n3y2cIKLjiOlp1Np37WiUdH\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n2HRqTpkrJO5ggZyyK6NPWz\nEspresso\n88.0\nShort n' Sweet\n2024-08-23\n175459.0\nSabrina Carpenter\nTrue\n4\n74KM79TiuVKeVCqs8QtB0B\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n6dOtVTDdiauQNBQEDOtlAB\nBIRDS OF A FEATHER\n96.0\nHIT ME HARD AND SOFT\n2024-05-17\n210373.0\nBillie Eilish\nFalse\n5\n6qqNVTkY8uBg9cP3Jd7DAH\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 146 columns\n\n\n\n\ndf_cl = df[['Track Name','Song Popularity','Album','Song Release Date','Duration (ms)','Artists','Explicit','Song Rank','Artist ID','Genres','Followers','Popularity','Lyrics','Total Artists On Song','Lyrics Word Count','Sentiment (VADER)']]\ndf_cl.loc[df_cl['Genres'].isna(), 'Genres'] = 'None Listed'\ndf_cl.loc[df_cl['Lyrics'] == \"Lyrics not found!\", 'Lyrics Word Count'] = 0\ndf_cl.loc[df_cl['Lyrics'] == \"Lyrics not found!\", 'Lyrics'] = ' '\ndf_cl = df_cl.dropna(subset='Lyrics')\ndf_cl.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 94 entries, 0 to 97\nData columns (total 16 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Track Name             94 non-null     object \n 1   Song Popularity        94 non-null     float64\n 2   Album                  94 non-null     object \n 3   Song Release Date      94 non-null     object \n 4   Duration (ms)          94 non-null     float64\n 5   Artists                94 non-null     object \n 6   Explicit               94 non-null     bool   \n 7   Song Rank              94 non-null     int64  \n 8   Artist ID              94 non-null     object \n 9   Genres                 94 non-null     object \n 10  Followers              94 non-null     int64  \n 11  Popularity             94 non-null     int64  \n 12  Lyrics                 94 non-null     object \n 13  Total Artists On Song  94 non-null     float64\n 14  Lyrics Word Count      94 non-null     int64  \n 15  Sentiment (VADER)      94 non-null     object \ndtypes: bool(1), float64(3), int64(4), object(8)\nmemory usage: 11.8+ KB\n\n\n\ndf_cl[\"Sentiment (VADER)\"] = df_cl[\"Sentiment (VADER)\"].apply(ast.literal_eval)\n\n# Expand the dictionary into separate columns\nsentiment_df = pd.json_normalize(df_cl[\"Sentiment (VADER)\"])\nsentiment_df = sentiment_df.reset_index(drop=True)\n\n# Combine the expanded columns back with the original DataFrame\ndf_cl = df_cl.reset_index(drop=True)\ndf_cl = pd.concat([df_cl.drop(columns=[\"Sentiment (VADER)\"]), sentiment_df], axis=1)\ndf_cl.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 94 entries, 0 to 93\nData columns (total 19 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Track Name             94 non-null     object \n 1   Song Popularity        94 non-null     float64\n 2   Album                  94 non-null     object \n 3   Song Release Date      94 non-null     object \n 4   Duration (ms)          94 non-null     float64\n 5   Artists                94 non-null     object \n 6   Explicit               94 non-null     bool   \n 7   Song Rank              94 non-null     int64  \n 8   Artist ID              94 non-null     object \n 9   Genres                 94 non-null     object \n 10  Followers              94 non-null     int64  \n 11  Popularity             94 non-null     int64  \n 12  Lyrics                 94 non-null     object \n 13  Total Artists On Song  94 non-null     float64\n 14  Lyrics Word Count      94 non-null     int64  \n 15  neg                    94 non-null     float64\n 16  neu                    94 non-null     float64\n 17  pos                    94 non-null     float64\n 18  compound               94 non-null     float64\ndtypes: bool(1), float64(7), int64(4), object(7)\nmemory usage: 13.4+ KB\n\n\n\n\nUnivariate Analysis\n\nSummary Statistics\nI ran .describe on the dataset to understand the distributiopn of values and decide if it was worth normalizing the data due to the wide range of possibilities for values for several of the columns. Duration, followers, and word count all range from very low to very high values, so it might be worth normalizing these columns before analysis.\n\ndf_cl.describe()\n\n\n\n\n\n\n\n\nSong Popularity\nDuration (ms)\nSong Rank\nFollowers\nPopularity\nTotal Artists On Song\nLyrics Word Count\nneg\nneu\npos\ncompound\n\n\n\n\ncount\n94.000000\n94.000000\n94.000000\n9.400000e+01\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.00000\n\n\nmean\n63.510638\n212252.319149\n49.372340\n1.163717e+07\n72.627660\n1.319149\n233.702128\n0.092543\n0.767011\n0.140436\n0.29302\n\n\nstd\n19.227973\n57749.294460\n29.438467\n2.373635e+07\n17.228374\n0.882479\n179.263177\n0.071248\n0.107798\n0.077664\n0.79927\n\n\nmin\n17.000000\n120792.000000\n1.000000\n1.273000e+03\n29.000000\n1.000000\n0.000000\n0.000000\n0.454000\n0.000000\n-0.99980\n\n\n25%\n48.250000\n175601.000000\n24.250000\n2.208655e+05\n60.000000\n1.000000\n132.500000\n0.034250\n0.701250\n0.093750\n-0.68960\n\n\n50%\n66.000000\n196731.500000\n48.500000\n1.710836e+06\n77.000000\n1.000000\n169.000000\n0.084000\n0.757500\n0.131000\n0.71875\n\n\n75%\n77.750000\n241314.500000\n75.500000\n8.641217e+06\n86.750000\n1.000000\n258.750000\n0.140750\n0.851000\n0.189750\n0.96500\n\n\nmax\n98.000000\n456933.000000\n100.000000\n1.280014e+08\n100.000000\n8.000000\n1160.000000\n0.373000\n1.000000\n0.426000\n0.99840\n\n\n\n\n\n\n\n\n\nHistograms\nThe next step is checking the distributions of all of my numeric variables to see if there’s anything interesting. Keeping in mind that these are considered the top 100 songs of the week that we chose, I analyzed the characteristics of the songs and their lyrics.\n\n# Create individual histograms for selected columns\ncolumns_to_plot = ['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound']\nfor column in columns_to_plot:\n    plt.figure(figsize=(6, 4))\n    plt.hist(df_cl[column], edgecolor='black', alpha=0.7)\n    plt.title(f\"Histogram for {column}\")\n    plt.xlabel(column)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above graphs, it is interesting to observe that the popularity of the song is not as heavily skewed as I thought. Since these are the top songs, I expected the spotify popularity score to be generally very high, but this was not the case. Additionally, it is interesting to see so many trending songs from artists without very many followers on Spotify. I think this is indicative of the popularity that Tik Tok and other social media platforms can provide for less known artists. Despite this, the popularity of the artists on Spotify seems to be heavily skewed. Moving on to analysis about characteristics about the lyrics, it appears that the songs are largely not negative and tend to be more neutral or positive. This can be seen by the histogram for the compound score as well, with this graph showing a large number of songs with a positive compound score.\n\n\nGenre Analysis\nI decided to group the really diverse genres into distinct categories. I used chatgpt to create the categories and asked it to give preference to genres that are not pop if multiple genres are included. Then I mapped the genre column to my new categories and made a histogram.\n\ndef map_genres_to_categories(genres):\n    # Define the mapping of specific genres to broader categories\n    genre_mapping = {\n        \"Pop\": [\n            \"pop\", \"social media pop\", \"dance pop\", \"modern country pop\", \"gen z singer-songwriter\", \"colombian pop\", \n            \"modern indie pop\", \"power pop\", \"la pop\", \"singer-songwriter pop\", \"uk pop\", \"alt z\"\n        ],\n        \"Indie/Alternative\": [\n            \"indie pop\", \"modern alternative pop\", \"indie rock\", \"small room\", \"asheville indie\", \"sacramento indie\", \n            \"twee pop\", \"tape club\", \"bubblegrunge\", \"bedroom pop\", \"slacker rock\", \"irish indie rock\", \"irish post-punk\"\n        ],\n        \"Rock\": [\n            \"album rock\", \"blues rock\", \"classic rock\", \"electric blues\", \"hard rock\", \"jam band\", \"southern rock\", \n            \"heartland rock\", \"garage rock\", \"modern blues rock\", \"modern rock\", \"punk blues\", \"noise rock\", \"no wave\"\n        ],\n        \"Country\": [\n            \"classic texas country\", \"contemporary country\", \"country dawn\", \"country road\", \"countrygaze\", \n            \"deep new americana\", \"roots americana\", \"classic oklahoma country\", \"red dirt\"\n        ],\n        \"Hip Hop/Rap\": [\n            \"hip hop\", \"rap\", \"west coast rap\", \"pop rap\", \"melodic rap\", \"houston rap\", \"atl hip hop\", \"trap\", \n            \"trap queen\", \"viral rap\", \"southern hip hop\", \"dfw rap\", \"canadian hip hop\", \"indian underground rap\", \n            \"desi hip hop\", \"malayalam hip hop\", \"irish hip hop\", \"conscious hip hop\"\n        ],\n        \"R&B\": [\n            \"r&b\", \"alternative r&b\", \"uk contemporary r&b\", \"afro r&b\"\n        ],\n        \"Latin\": [\n            \"reggaeton\", \"urbano latino\", \"trap latino\", \"reggaeton chileno\", \"reggaeton colombiano\", \"latin pop\"\n        ],\n        \"Afrobeat/African\": [\n            \"afrobeats\", \"afropop\", \"azonto\", \"nigerian pop\", \"alte\", \"nigerian hip hop\"\n        ],\n        \"Electronic/Experimental\": [\n            \"art pop\", \"metropopolis\", \"ambient folk\", \"freak folk\", \"hyperpop\", \"proto-hyperpop\", \"bubblegum bass\", \n            \"digital hardcore\", \"escape room\", \"experimental pop\", \"experimental hip hop\", \"deconstructed club\", \n            \"electronica\", \"glitch\", \"glitch hop\", \"jazztronica\", \"intelligent dance music\", \"psychedelic hip hop\", \n            \"wonky\", \"indietronica\", \"afrofuturism\", \"transpop\", \"uk alternative pop\", \"crank wave\", \"jersey club\"\n        ],\n        \"K-pop\": [\n            \"k-pop\", \"k-pop girl group\", \"anime\"\n        ],\n        \"Other\": [\n            \"None Listed\"\n        ]\n    }\n\n    for category, genre_list in genre_mapping.items():\n        if genres.split(\", \")[0] in genre_list:\n            return category\n        elif len(genres.split(\",\"))&gt;1:\n            if genres.split(\", \")[1] in genre_list:\n                return category\n    return \"Other\"\n\ndf_cl['Simplified Genre'] = df_cl['Genres'].apply(map_genres_to_categories)\ndf_cl[['Genres','Simplified Genre']].head()\n\n\n\n\n\n\n\n\nGenres\nSimplified Genre\n\n\n\n\n0\nindie pop\nIndie/Alternative\n\n\n1\nconscious hip hop, hip hop, rap, west coast rap\nHip Hop/Rap\n\n\n2\nmodern country pop, pop rap\nPop\n\n\n3\npop\nPop\n\n\n4\nart pop, pop\nPop\n\n\n\n\n\n\n\n\nplt.hist(df_cl['Simplified Genre'], edgecolor='black', alpha=0.7)\nplt.title(f\"Histogram for Simplified Genre\")\nplt.xlabel(\"Simplified Genre\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nFrom this, we can see that pop clearly dominates the top of the charts, but there is still a lot of diversity in the genre.\n\nplt.hist(df_cl['Explicit'].astype(int), edgecolor='black', alpha=0.7)\nplt.title(f\"Histogram for Explicit Songs\")\nplt.xlabel(\"Explicit\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=90) \nplt.show()\n\n\n\n\n\n\n\n\nMost songs are not explicit but some of them are.\n\n\n\nBivariate and Multivariate Analysis\n\n# Create a new column with bins of size 10\ndf_cl[\"Ranking Group\"] = pd.cut(\n    df_cl[\"Song Rank\"], \n    bins=range(0, 101, 10),\n    labels=[f\"{i+1}-{i+10}\" for i in range(0, 100, 10)],\n    right=True\n)\ndf_cl[\"Top 10 Song\"] = df_cl[\"Song Rank\"].apply(lambda x: 1 if x &lt;= 10 else 0)\ndf_cl[[\"Ranking Group\", \"Song Rank\", \"Top 10 Song\"]]\n\n\n\n\n\n\n\n\nRanking Group\nSong Rank\nTop 10 Song\n\n\n\n\n0\n1-10\n1\n1\n\n\n1\n1-10\n2\n1\n\n\n2\n1-10\n3\n1\n\n\n3\n1-10\n4\n1\n\n\n4\n1-10\n5\n1\n\n\n...\n...\n...\n...\n\n\n89\n91-100\n96\n0\n\n\n90\n91-100\n97\n0\n\n\n91\n91-100\n98\n0\n\n\n92\n91-100\n99\n0\n\n\n93\n91-100\n100\n0\n\n\n\n\n94 rows × 3 columns\n\n\n\n\nsns.pairplot(df_cl[['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound', 'Ranking Group']], hue=\"Ranking Group\")\n\n\n\n\n\n\n\n\nThe above graph is a pairplot of the numeric columns and the hue is the ranking groups that I made to bin ranking by groups of 10. I’m not seeing any clear clusters appearing here between any of the variables. There is a positive linear relationship between artist popularity and song popularity though. This might be expected due to the way that spotify calculates this popularity and this high correlation might mean we should remove one of these variables when creating models.\n\nsns.pairplot(df_cl[['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Total Artists On Song', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound', 'Top 10 Song']], hue=\"Top 10 Song\")\n\n\n\n\n\n\n\n\nThis is a pairplot where only the top 10 songs are colored. There aren’t many clear distinctions but I am curious about the clustering in the scatter plot between song popularity and song duration as well as song popularity and positive sentiment score.\n\ncorr = df_cl[['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', \n              'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound']].corr()\n\n# Plot the correlation matrix\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nThis correlation plot confirms my earlier observations about popularity and song popularity being highly correlated. It also shows the relationships between the various outputs of VADER which is to be expected and might require that we only use one of the 4 output scores during analysis. There is also the obvious correlation between lyrics word count and duration, although I expected this to be much higher. I think the slight correlation between negative lyric sentiment and lyric word count is interesting. There is also a slight negative correlation between positive lyric sentiment and song duration.\n\ngenre_counts = df_cl.groupby(['Ranking Group', 'Simplified Genre'], observed=True).size().unstack()\ngenre_counts.plot(kind='bar', stacked=True, figsize=(10,6))\nplt.title(\"Genre Distribution by Ranking Group\")\nplt.xlabel(\"Ranking Group\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nThis is a breakdown of the genres by ranking group. The distribution of genres seems to be unchanged across the different ranking groups (as in there is a mix of genres across each group). However, some genres don’t appear at all in the top 20, including K-Pop and R&B.\n\nsns.boxplot(data=df_cl, x='Ranking Group', y='Lyrics Word Count')\nplt.title(\"Lyrics Word Count by Ranking Group\")\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\nThis is a breakdown of the word count by group, with the highest and lowest word count groups not being in the top 10.\n\nsns.boxplot(data=df_cl, x='Ranking Group', y='compound')\nplt.title(\"Compound by Ranking Group\")\nplt.show()\n\n\n\n\n\n\n\n\nThis is a graph of the compound score by ranking group. Overall, the median seems to increase as the rankings increase, possibly indicating that songs that are ranked higher are less positive (or even negative).\n\nsns.scatterplot(x='Duration (ms)', y='Song Popularity', hue='Top 10 Song', data=df_cl)\nplt.title(\"Song Popularity vs. Duration (ms) by Top 10 Song\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows the Duration of the song vs it’s popularity. It seems that the most popular songs are between 150000 and 275000 ms, with most of the top 10 songs falling between 175000 and 225000.\n\nsns.scatterplot(x='Lyrics Word Count', y='Popularity', hue='Top 10 Song', data=df_cl)\nplt.title(\"Popularity vs. Lyrics Word Count by Ranking Group\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatter plot shows the word count of the song vs it’s popularity. Most of the top 10 songs are concentrated between 150 and 300, with only 2 outliers for this.\n\n# Create crosstab for Ranking Group vs Explicit\nexplicit_ranking_crosstab = pd.crosstab(df_cl['Ranking Group'], df_cl['Explicit'])\n\n# Plot a grouped bar plot\nexplicit_ranking_crosstab.plot(kind='bar', stacked=False)\nplt.title('Ranking Group vs Explicit')\nplt.xlabel('Ranking Group')\nplt.ylabel('Count')\nplt.xticks(rotation=90)\nplt.legend(title='Explicit')\nplt.show()\n\n\n\n\n\n\n\n\nThis graph shows the distribution of explicit songs in each category. For the middle categories, it is interesting that there is a perfect split between explicit and not explicit songs. The top category has far more non explicit songs compared to explicit songs. This is also true of the bottom categories, however, so this might not be the most predictive feature.\n\n# Create crosstab for Top 10 Song vs Simplified Genre\ntop10_genre_crosstab = pd.crosstab(df_cl['Top 10 Song'], df_cl['Simplified Genre'])\n\n# Plot a grouped bar plot\ntop10_genre_crosstab.plot(kind='bar', stacked=False)\nplt.title('Top 10 Song vs Simplified Genre')\nplt.xlabel('Top 10 Song')\nplt.ylabel('Count')\nplt.legend(title='Simplified Genre')\nplt.show()\n\n\n\n\n\n\n\n\nThis is another look at the distribution of genres between top 19 songs and not top 10 songs. There are a lot of cateogries missing from the top 10 and the heaviest weight is on pop and indie, which could be a good predictor of the top songs.\n\n\nData Distribution and Normalization\n\ncontinuous_columns = ['Song Popularity', 'Duration (ms)', 'Followers', 'Popularity', 'Lyrics Word Count', 'neg', 'neu', 'pos', 'compound']\n\n# Create histograms and density plots for each continuous column\nplt.figure(figsize=(15, 10))\n\nfor i, column in enumerate(continuous_columns, 1):\n    plt.subplot(3, 3, i)  # 3 rows, 4 columns\n    sns.histplot(df_cl[column], kde=True, bins=30, color='skyblue')\n    plt.title(f'Distribution of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis is a figure for a histogram and density plot for each of the continuous variables of interest.\n\nSong popularity: roughly bell shaped, some skewing, most songs between 40-90 - relatively evenly distributed with a more songs having a higher popularity\nDuration: high right skew, most songs are shorter (2.5-3 minutes) with very few songs having longer durations\nFollowers: highly skewed, few artists have a large number of followers while most have low follower counts\nArtist Popularity: rough bell curve, somewhat skewed, concentrated in 70-90 range, most songs come from popular artists\nWord Count: most have fewer than 200 words, right skew\nNegative sentiment: Relatively even spread from 0-0.3, most have low neg sentiment\nNetural sentiment: most between 0.7-0.9, pretty even, most have medium to high neutral\nPositive sentimet: slightly bell shaped, right skew, scores between 0.05-0.15, pretty low pos sentiment overall\nCompound: mis of pos and neg values but most are closer to 0 than extremes (except for positive 1)\n\nMost of the variables are skewed. Some variable might be normal-like but are still skewed.\n\nfrom scipy.stats import skew, kurtosis\n\nskewness = df_cl[continuous_columns].apply(skew)\nkurt = df_cl[continuous_columns].apply(kurtosis)\n\nprint(\"Skewness:\")\nprint(skewness)\nprint(\"\\nKurtosis:\")\nprint(kurt)\n\nSkewness:\nSong Popularity     -0.377718\nDuration (ms)        1.665246\nFollowers            3.097444\nPopularity          -0.588679\nLyrics Word Count    2.480521\nneg                  1.076954\nneu                 -0.235075\npos                  0.610432\ncompound            -0.647878\ndtype: float64\n\nKurtosis:\nSong Popularity     -0.671813\nDuration (ms)        4.075121\nFollowers            9.910030\nPopularity          -0.565867\nLyrics Word Count    7.537863\nneg                  1.705756\nneu                  0.266109\npos                  1.037166\ncompound            -1.363075\ndtype: float64\n\n\nThe skewness and kurtosis test results confirm the above findings. Followers, Duration, and Lyrics Word Count all have a highly positive kurtosis, indicating a lot of outliers.\n\n# Apply log transformation to variables with high skewness\ndf_cl['Duration (ms)_transf'] = np.log1p(df_cl['Duration (ms)'])  # log1p handles zero values (log(1+x))\ndf_cl['Followers_transf'] = np.log1p(df_cl['Followers'])  # log1p handles zero values\ndf_cl['Lyrics Word Count_transf'] = np.log1p(df_cl['Lyrics Word Count'])  # log1p handles zero values\n\n# Check the transformations\ndf_cl[['Duration (ms)', 'Followers', 'Lyrics Word Count']].describe()\n\n\n\n\n\n\n\n\nDuration (ms)\nFollowers\nLyrics Word Count\n\n\n\n\ncount\n94.000000\n9.400000e+01\n94.000000\n\n\nmean\n212252.319149\n1.163717e+07\n233.702128\n\n\nstd\n57749.294460\n2.373635e+07\n179.263177\n\n\nmin\n120792.000000\n1.273000e+03\n0.000000\n\n\n25%\n175601.000000\n2.208655e+05\n132.500000\n\n\n50%\n196731.500000\n1.710836e+06\n169.000000\n\n\n75%\n241314.500000\n8.641217e+06\n258.750000\n\n\nmax\n456933.000000\n1.280014e+08\n1160.000000\n\n\n\n\n\n\n\n\ncontinuous_columns = ['Song Popularity', 'Duration (ms)_transf', 'Followers_transf', 'Popularity', 'Lyrics Word Count_transf', 'neg', 'neu', 'pos', 'compound']\n\n# Create histograms and density plots for each continuous column\nplt.figure(figsize=(15, 10))\n\nfor i, column in enumerate(continuous_columns, 1):\n    plt.subplot(3, 3, i)  # 3 rows, 4 columns\n    sns.histplot(df_cl[column], kde=True, bins=30)\n    plt.title(f'Distribution of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nI decided to apply a log transformation to Duration, Followers, and Word Count so that their distributions would be much less skewed. After looking at the histograms and density plots, I can see that this in fact helped the skewness and helped make the data more normal. Doing this will allow for clustering models to fit better on the data and make for a more clear insights about the data. It also decreases the outliers that’ll impact the model.\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\ndf_cl['Popularity'] = df_cl['Popularity']/100\ndf_cl['Song Popularity'] = df_cl['Song Popularity']/100\n\n# Selecting columns that were transformed\ncolumns_to_normalize = ['Duration (ms)_transf', 'Followers_transf', 'Lyrics Word Count_transf']\n\n# Initialize scalers\nmin_max_scaler = MinMaxScaler()\nstandard_scaler = StandardScaler()\n\n# Apply Min-Max Scaling\ndf_cl_minmax = df_cl[columns_to_normalize].copy()\ndf_cl_minmax[columns_to_normalize] = min_max_scaler.fit_transform(df_cl_minmax[columns_to_normalize])\n\n# Plotting the results\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Original distributions\nfor i, column in enumerate(columns_to_normalize):\n    sns.histplot(df_cl[column], kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f\"Original Distribution of {column}\")\n    \n# Normalized distributions\nfor i, column in enumerate(columns_to_normalize):\n    sns.histplot(df_cl_minmax[column], kde=True, ax=axes[1, i])\n    axes[1, i].set_title(f\"Min-Max Scaled Distribution of {column}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_cl[columns_to_normalize] = df_cl_minmax[columns_to_normalize]\ndf_cl.describe()\n\n\n\n\n\n\n\n\nSong Popularity\nDuration (ms)\nSong Rank\nFollowers\nPopularity\nTotal Artists On Song\nLyrics Word Count\nneg\nneu\npos\ncompound\nTop 10 Song\nDuration (ms)_transf\nFollowers_transf\nLyrics Word Count_transf\n\n\n\n\ncount\n94.000000\n94.000000\n94.000000\n9.400000e+01\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.000000\n94.00000\n94.000000\n94.000000\n94.000000\n94.000000\n\n\nmean\n0.635106\n212252.319149\n49.372340\n1.163717e+07\n0.726277\n1.319149\n233.702128\n0.092543\n0.767011\n0.140436\n0.29302\n0.106383\n0.400155\n0.613816\n0.741318\n\n\nstd\n0.192280\n57749.294460\n29.438467\n2.373635e+07\n0.172284\n0.882479\n179.263177\n0.071248\n0.107798\n0.077664\n0.79927\n0.309980\n0.183601\n0.213827\n0.110045\n\n\nmin\n0.170000\n120792.000000\n1.000000\n1.273000e+03\n0.290000\n1.000000\n0.000000\n0.000000\n0.454000\n0.000000\n-0.99980\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.482500\n175601.000000\n24.250000\n2.208655e+05\n0.600000\n1.000000\n132.500000\n0.034250\n0.701250\n0.093750\n-0.68960\n0.000000\n0.281211\n0.447585\n0.693480\n\n\n50%\n0.660000\n196731.500000\n48.500000\n1.710836e+06\n0.770000\n1.000000\n169.000000\n0.084000\n0.757500\n0.131000\n0.71875\n0.000000\n0.366607\n0.625233\n0.727753\n\n\n75%\n0.777500\n241314.500000\n75.500000\n8.641217e+06\n0.867500\n1.000000\n258.750000\n0.140750\n0.851000\n0.189750\n0.96500\n0.000000\n0.520095\n0.765952\n0.787792\n\n\nmax\n0.980000\n456933.000000\n100.000000\n1.280014e+08\n1.000000\n8.000000\n1160.000000\n0.373000\n1.000000\n0.426000\n0.99840\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nUsing min-max scaling on these 3 columns changes them from a scale on the hundreds of thousands to a scale from 0 to 1. This will make sure that these features don’t disproportionately have a bigger influence on a model. It also puts these features on an equal scale as all of the other features which will be very beneficial when running clustering algorithms.\n\ndf_cl.to_csv('../../data/processed-data/transformed_data.csv')\n\n\n\nStatistical Insights\n\nfrom scipy.stats import ttest_ind\n\n# Split data into two groups\ntop_10 = df_cl[df_cl['Top 10 Song'] == 1]['Followers_transf']\nnot_top_10 = df_cl[df_cl['Top 10 Song'] == 0]['Followers_transf']\n\n# Perform independent samples T-test\nt_stat, p_val = ttest_ind(top_10, not_top_10, equal_var=False)  # Welch's T-test\n\n# Output results\nprint(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4f}\")\n\nif p_val &lt; 0.05:\n    print(\"The difference in Followers between Top 10 Songs and others is statistically significant.\")\nelse:\n    print(\"No significant difference in Followers between Top 10 Songs and others.\")\n\nT-statistic: 2.4122, P-value: 0.0303\nThe difference in Followers between Top 10 Songs and others is statistically significant.\n\n\n\nfrom scipy.stats import f_oneway\n\n# Group data by Ranking Group\ngroup_1 = df_cl[df_cl['Ranking Group'] == '1-10']['Lyrics Word Count_transf']\ngroup_2 = df_cl[df_cl['Ranking Group'] == '11-20']['Lyrics Word Count_transf']\ngroup_3 = df_cl[df_cl['Ranking Group'] == '21-30']['Lyrics Word Count_transf']\ngroup_4 = df_cl[df_cl['Ranking Group'] == '31-40']['Lyrics Word Count_transf']\n\n# Perform ANOVA\nf_stat, p_val = f_oneway(group_1, group_2, group_3, group_4)\n\n# Output results\nprint(f\"F-statistic: {f_stat:.4f}, P-value: {p_val:.4f}\")\n\nif p_val &lt; 0.05:\n    print(\"There are significant differences in Song Popularity across Ranking Groups.\")\nelse:\n    print(\"No significant differences in Song Popularity across Ranking Groups.\")\n\nF-statistic: 0.4504, P-value: 0.7186\nNo significant differences in Song Popularity across Ranking Groups."
  },
  {
    "objectID": "technical-details/eda/main.html#conclusions-and-next-steps",
    "href": "technical-details/eda/main.html#conclusions-and-next-steps",
    "title": "Exploratory Data Analysis",
    "section": "Conclusions and Next Steps",
    "text": "Conclusions and Next Steps\nSummarize your findings, interpret the results, and discuss their technical implications. - Summary of EDA Findings: - Highlight the main takeaways from the EDA process (key trends, patterns, data quality issues). - Implications for Modeling: - Discuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations). - Outline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is a critical step in ensuring the quality and usability of data for analysis. In this project, the data cleaning process involved merging data frames, creating more-indepth columns, handling missing variables, and coverting data types.\nSince we want to analyze artists, songs, and lyrics, we conducted several changes such as adding song ranking, one hot encoding the genres, conducting sentiment analysis on the lyrics, and looking into artists features in a song. These will help streamline the EDA process as well as our analysis."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-to-data-cleaning",
    "href": "technical-details/data-cleaning/main.html#introduction-to-data-cleaning",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data cleaning is a critical step in ensuring the quality and usability of data for analysis. In this project, the data cleaning process involved merging data frames, creating more-indepth columns, handling missing variables, and coverting data types.\nSince we want to analyze artists, songs, and lyrics, we conducted several changes such as adding song ranking, one hot encoding the genres, conducting sentiment analysis on the lyrics, and looking into artists features in a song. These will help streamline the EDA process as well as our analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "Spotify API: Used to collect information about songs and artists.\n\nSpotify API Documentation\n\nGenius API: Used to retrieve song lyrics.\n\nGenius API Documentation\n\n\nSpotify API provides detailed metada about tracks and artists, which is important as we want to look at the song details, artist details, and popularity. Genius API provies lyrics which can be used for word analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-source-information",
    "href": "technical-details/data-collection/main.html#data-source-information",
    "title": "Data Collection",
    "section": "",
    "text": "Spotify API: Used to collect information about songs and artists.\n\nSpotify API Documentation\n\nGenius API: Used to retrieve song lyrics.\n\nGenius API Documentation\n\n\nSpotify API provides detailed metada about tracks and artists, which is important as we want to look at the song details, artist details, and popularity. Genius API provies lyrics which can be used for word analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-method",
    "href": "technical-details/data-collection/main.html#data-collection-method",
    "title": "Data Collection",
    "section": "Data Collection Method",
    "text": "Data Collection Method\n\nSpotify API\n\nUsed spotipy library to authenticate client ID and client secret.\nAfter pulling the songs from Rolling Stone’s spotify Top 100 playlist, we pullled all the information of the artists and the tracks into seperate csvs.\n\nGenius API\n\nAuthenticated using client access token.\nGot the lyrics url for each song by querying Genius for song details.\nUsed BeautifulSoup to scrape lyrics from the url.\n\nBeautifulSoup is a Python library used for web scraping. It allows us to get data from HTML and XML files."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-structure-and-format",
    "href": "technical-details/data-collection/main.html#data-structure-and-format",
    "title": "Data Collection",
    "section": "Data Structure and Format",
    "text": "Data Structure and Format\n\nartists_data.csv\n\nRows represent individual artists with columns: Artist ID, Name, Genres, Followers, Popularity.\n\nrolling_stone_top_100,csv\n\nRows represent each song with columns: Track Name, Artists.\n\nsong_lyrics.csv\n\nRows represent each song with columns: Track Name, Artists, lyrics.\n\ntracks_data.csv\n\nRows represent each song with columns: Track ID, Track Name, Popularity, Album, Release Date, Duration (ms), Artist, Explicit."
  },
  {
    "objectID": "technical-details/data-collection/main.html#linking-to-data",
    "href": "technical-details/data-collection/main.html#linking-to-data",
    "title": "Data Collection",
    "section": "Linking to Data",
    "text": "Linking to Data\n\nartists_data.csv\nrolling_stone_top_100,csv\nsongs_lyrics.csv\ntracks_data.csv"
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "Goals",
    "text": "Goals\nThe primary goal of this notebook is to colelct, process and analyze song metadata and lyrics from multiple APIS (Spotify and Genius) to analyze song popularity and its predictors. We can acheive this by acuiring data from songs that are already considered the top 100 of 2024, cleaning the dataset, conduting exploratory data analysis, machine learning analysis, and predictive analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "Motivation",
    "text": "Motivation\nThis work is motivated by the need to bridge the gap between song qualities, artist features, and their impact on song popularity. By combining metadata with lyrical analysis, we aim to illustrate patterns that can better enhance music recommendations, audience targeting strategies, and creative decision-making for artists."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "Objectives",
    "text": "Objectives\n\nRetrieve song and artist details using the Spotify API.\nExtract lyrics using the Genius API and BeautifulSoup.\nNormalize and clean the collected data.\nStructure datasets for exploratory data analysis (EDA) and machine learning workflows.\nIdentify key features."
  },
  {
    "objectID": "technical-details/data-collection/main.html#methods",
    "href": "technical-details/data-collection/main.html#methods",
    "title": "Data Collection",
    "section": "Methods",
    "text": "Methods\nThis project utilizes a combination of API, web scraping, and data processing techniques to collect and prepare song metadata and lyrics for analysis.\n\nData Collection: Song metadata was retrieved using the Spotify API with spotipy, while lyrics were obtained through the Genius API and scraped with BeautifulSoup.\nData Processing: Data was cleaned, normalized, and merged.\nTools Used: Libraries such as spotipy, requests, BeautifulSoup, and pandas were employed for API interaction, HTML parsing, and data manipulation.\n\nThese methods provides a foundation for preparing the data for analysis and modeling tasks."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nData Retrieval:\n\nSpotify API: Spotify API does not let you handle more than 50 requests.\nGenius API: Inconsistencies in matching song titles and artists between Spotify and Genius.\n\nWeb Scraping:\n\nPage structures on Genius required fine-tuning of BeautifulSoup parsing logic to extract lyrics.\nSome songs were purely instrumental so lyrics are missing.\n\nData Integration:\n\nDifferences in naming conventions and formats across APIs required additional care.\nSince some songs have artists features, the amount of artists extracted are more than the amount of songs extracted."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nSpotify’s popularity scores align with expected trends, serving as a basis for analysis.\nSpotify metadata and Genius lyrics offers a strong foundation for future analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion",
    "href": "technical-details/data-collection/main.html#conclusion",
    "title": "Data Collection",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nThe project successfully gathered song metadata and lyrics.\nThe resulting datasets are comprehensive and ready for exploratory data analysis, clustering, and predictive modeling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#future-steps",
    "href": "technical-details/data-collection/main.html#future-steps",
    "title": "Data Collection",
    "section": "Future Steps:",
    "text": "Future Steps:\n\nDevelop solutions for handling mismatched data between Spotify and Genius.\nInclude additional playlists or genres for in-depth analysis.\nConduct sentiment analysis on song lyrics and compare results with Spotify’s popularity scores.\nConduct machine learning models to identify patterns and predictors of song popularity."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\n\n\n\nDefine the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\n\n\n\nPresent the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\n\n\n\nUse clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\n\n\n\nExplain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\n\n\n\nProvide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\n\n\n\nSummarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\n\n\n\n(Optional): Additional charts or explanations for further insights.\n\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#executive-summary",
    "href": "report/report.html#executive-summary",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”"
  },
  {
    "objectID": "report/report.html#objective",
    "href": "report/report.html#objective",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”"
  },
  {
    "objectID": "report/report.html#key-insights",
    "href": "report/report.html#key-insights",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”"
  },
  {
    "objectID": "report/report.html#visualizations",
    "href": "report/report.html#visualizations",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement."
  },
  {
    "objectID": "report/report.html#business-implications",
    "href": "report/report.html#business-implications",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”"
  },
  {
    "objectID": "report/report.html#recommendations",
    "href": "report/report.html#recommendations",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”"
  },
  {
    "objectID": "report/report.html#conclusion",
    "href": "report/report.html#conclusion",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”"
  },
  {
    "objectID": "report/report.html#appendix",
    "href": "report/report.html#appendix",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "(Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Decoding Song Success - Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Music, Lyrics, Hot 100 chart, hit songs, popular music research, Natural Language Processing, Supervised Machine Learning, Unsupervised Machine Learning, Sentiment Analysis\n\n\n\nDecoding Song Success uses data science to uncover insights from the Rolling Stones Top 100 Songs. We bridged NLP and song metadata to analyze features like the characteristics of the lyrics of hits by running sentiment analysis, the popularity of the song, the popularity of the artist, and the genre. We hope to investigate how these features interact to shape the top hits. This website offers a data driven report, informative visualizations, and predictive models to convey the makings of a hit song.\n\n\nMusic is an important part of popular culture with implications in everything from social contexts to consumerism. What makes a song a hit lies in complex emotional and commercial relationships. Analyzing the data about the songs that are considered successful can reveal connections between the lyrical sentiment, the song’s characteristics, and audience preferences. This project highlights these patterns to help inform rising artists and predict commerciall success in the music industry. Some use cases of this analysis include helping songwriters decide whether explicit songs differ significantly from not explicit songs, cluster songs by features to offer recommendations, and help industry professionals predict whether a song will be a hit as they make investment decisions.\n\n\n\nThe overall driving question of this exploration is as follows:\nWhat makes a song more popular with audiences, and how do lyrical and musical factors shape its success?\n\n\n\nAs we conducted our investigation of music data, we aimed to answer the following questions:\n\nHow does lyric sentiment influence a song’s popularity? Do other lyric features (such as length) have an influence?\nDoes lyric sentiment differ across genres, and does the trend between sentiment and popularity change between genres?\nHow does profanity in a song impact it’s sentiment and popularity?\nCan we use unsupervised learning techniques to identify clusters and create playlists of songs that someone may like based on liking a hit song?\nDoes collaboration between artists produce more popular songs? How does individual artist popularity impact this?\n\n\n\n\nThis project provides an in-depth study of the connection between data science and music. As you explore the visualizations, predictive models, and business report, we hop you learn more about what makes a song a hit!\n\n\n\n\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\n\nTo conduct this literature review, we decided to split articles into two categories: research about the musicality of songs in top charts and papers about the lyrics of songs in top charts. While the first topic has a wide range of approaches, studies related to the lyrics are far rarer. 3 articles were found for each category and the results are reported below.\n\n\n\n\nIn this research article, Interiano et al. uses random forest models to predict whether a song would “make it to the charts” using acoustic features such as mood, danceability, relaxedness, and frequency of male/female singers contributing to the song. They highlighted the trend that songs with fewer male contributors, instrumentals that are considered more “dance-based” music, and brighter sounding songs are strong indicators of the success of a song. The graph below shows this trend over time, with the red dots being songs that are not in the top 100 and blue dots being songs that are in the top 100.\n\n\n\nSource 1 Results\n\n\n\n\n\nKim et al. developed a hierarchical Bayesian logit choice model that takes into account features of the instrumentals of a range of hit songs (such as melody, tempo, harmony, and rhythm) to determine the consumer preference for a specific song. The data was collected through controlled lab tests. They then tried to correlate the resulting preferences to the profits of a song by considering music development costs. Overall, they found that consonant harmony, piano, slow tempo, and irregular rhythm to be more desirable by consumers and to be the most monetarily sustainable choices to include in a song.\n\n\n\nHong analyzed songs that have hit the number 1 spot from 1955 to 2003 and used features such as gender, number, and race of the singers of the song to analyze if artist characteristics contributed to the long-term success of a song (i.e. number of weeks that the song would be at the top of the charts). They used parametric estimation of a log-logistic distribution to find that the presence of Black artists contributed to longer runs at the top of the chart (which they contribute to the success of rap songs). They also concluded that, in groups, the gender of the members of the groups did not actually contribute to the song’s success.\n\n\n\n\n\n\nPettijohn et al. conducted an analysis on the changes in the subjects of the lyrics of Billboard number 1 songs between 1955-2003 using the LIWC program for NLP which produces a series of psychometric features of songs, such as a focus on the future, meaningfulness, comfort, and romance. They then correlated this with US economic conditions collected from various psychology studies regarding the social reaction to US presidents and politics. In the end, they found that songs that were more romantic and comforting, as well as songs with more words per sentence and more mentions of social processes were more common during years that they considered to have more “threatening” conditions.\n\n\n\nStudy 3 in the paper by Nunes et al. considered songs from Billboard’s top 100 singles between 1958-2012 and studied how repetitive they are (such as how many times the chorus is repeated). They determined that more repetitive lyrics correlate strongly with the position at which a song debuts (top 40 or not) and how many weeks it takes for the song to reach the number 1 spot. This understanding of the structure of lyrics is a novel approach to studying the popularity of music and informed some of the questions we asked about our dataset.\n\n\n\nThis report by Tough introduces a new set of features regarding the songwriting procedures of Billboard Hot 100 songs between 2014-2015, including repeating the title in the song, intro length, the presence of archetypes (i.e. the Innocent, the Lover, the Everyman, the Ruler), and song subject. The results include showing that a song in the top 100 is more likely to have an 11-15 second intro, start with a chorus or hook, and have the lover archetype (followed by the warrior archetype). They also find that specific combinations of archetypes (including the warrior/ruler pairing) are more common in the top 100 songs. The below charts show some of these results.\n\n\n\nSource 6 Results - Intro Length\n\n\n\n\n\nSource 6 Results - Intro Length"
  },
  {
    "objectID": "index.html#key-topics",
    "href": "index.html#key-topics",
    "title": "Landing page",
    "section": "",
    "text": "Music, Lyrics, Hot 100 chart, hit songs, popular music research, Natural Language Processing, Supervised Machine Learning, Unsupervised Machine Learning, Sentiment Analysis"
  },
  {
    "objectID": "index.html#project-introduction",
    "href": "index.html#project-introduction",
    "title": "Landing page",
    "section": "",
    "text": "Decoding Song Success uses data science to uncover insights from the Rolling Stones Top 100 Songs. We bridged NLP and song metadata to analyze features like the characteristics of the lyrics of hits by running sentiment analysis, the popularity of the song, the popularity of the artist, and the genre. We hope to investigate how these features interact to shape the top hits. This website offers a data driven report, informative visualizations, and predictive models to convey the makings of a hit song.\n\n\nMusic is an important part of popular culture with implications in everything from social contexts to consumerism. What makes a song a hit lies in complex emotional and commercial relationships. Analyzing the data about the songs that are considered successful can reveal connections between the lyrical sentiment, the song’s characteristics, and audience preferences. This project highlights these patterns to help inform rising artists and predict commerciall success in the music industry. Some use cases of this analysis include helping songwriters decide whether explicit songs differ significantly from not explicit songs, cluster songs by features to offer recommendations, and help industry professionals predict whether a song will be a hit as they make investment decisions.\n\n\n\nThe overall driving question of this exploration is as follows:\nWhat makes a song more popular with audiences, and how do lyrical and musical factors shape its success?\n\n\n\nAs we conducted our investigation of music data, we aimed to answer the following questions:\n\nHow does lyric sentiment influence a song’s popularity? Do other lyric features (such as length) have an influence?\nDoes lyric sentiment differ across genres, and does the trend between sentiment and popularity change between genres?\nHow does profanity in a song impact it’s sentiment and popularity?\nCan we use unsupervised learning techniques to identify clusters and create playlists of songs that someone may like based on liking a hit song?\nDoes collaboration between artists produce more popular songs? How does individual artist popularity impact this?\n\n\n\n\nThis project provides an in-depth study of the connection between data science and music. As you explore the visualizations, predictive models, and business report, we hop you learn more about what makes a song a hit!"
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Landing page",
    "section": "",
    "text": "Develop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Landing page",
    "section": "",
    "text": "To conduct this literature review, we decided to split articles into two categories: research about the musicality of songs in top charts and papers about the lyrics of songs in top charts. While the first topic has a wide range of approaches, studies related to the lyrics are far rarer. 3 articles were found for each category and the results are reported below.\n\n\n\n\nIn this research article, Interiano et al. uses random forest models to predict whether a song would “make it to the charts” using acoustic features such as mood, danceability, relaxedness, and frequency of male/female singers contributing to the song. They highlighted the trend that songs with fewer male contributors, instrumentals that are considered more “dance-based” music, and brighter sounding songs are strong indicators of the success of a song. The graph below shows this trend over time, with the red dots being songs that are not in the top 100 and blue dots being songs that are in the top 100.\n\n\n\nSource 1 Results\n\n\n\n\n\nKim et al. developed a hierarchical Bayesian logit choice model that takes into account features of the instrumentals of a range of hit songs (such as melody, tempo, harmony, and rhythm) to determine the consumer preference for a specific song. The data was collected through controlled lab tests. They then tried to correlate the resulting preferences to the profits of a song by considering music development costs. Overall, they found that consonant harmony, piano, slow tempo, and irregular rhythm to be more desirable by consumers and to be the most monetarily sustainable choices to include in a song.\n\n\n\nHong analyzed songs that have hit the number 1 spot from 1955 to 2003 and used features such as gender, number, and race of the singers of the song to analyze if artist characteristics contributed to the long-term success of a song (i.e. number of weeks that the song would be at the top of the charts). They used parametric estimation of a log-logistic distribution to find that the presence of Black artists contributed to longer runs at the top of the chart (which they contribute to the success of rap songs). They also concluded that, in groups, the gender of the members of the groups did not actually contribute to the song’s success.\n\n\n\n\n\n\nPettijohn et al. conducted an analysis on the changes in the subjects of the lyrics of Billboard number 1 songs between 1955-2003 using the LIWC program for NLP which produces a series of psychometric features of songs, such as a focus on the future, meaningfulness, comfort, and romance. They then correlated this with US economic conditions collected from various psychology studies regarding the social reaction to US presidents and politics. In the end, they found that songs that were more romantic and comforting, as well as songs with more words per sentence and more mentions of social processes were more common during years that they considered to have more “threatening” conditions.\n\n\n\nStudy 3 in the paper by Nunes et al. considered songs from Billboard’s top 100 singles between 1958-2012 and studied how repetitive they are (such as how many times the chorus is repeated). They determined that more repetitive lyrics correlate strongly with the position at which a song debuts (top 40 or not) and how many weeks it takes for the song to reach the number 1 spot. This understanding of the structure of lyrics is a novel approach to studying the popularity of music and informed some of the questions we asked about our dataset.\n\n\n\nThis report by Tough introduces a new set of features regarding the songwriting procedures of Billboard Hot 100 songs between 2014-2015, including repeating the title in the song, intro length, the presence of archetypes (i.e. the Innocent, the Lover, the Everyman, the Ruler), and song subject. The results include showing that a song in the top 100 is more likely to have an 11-15 second intro, start with a chorus or hook, and have the lover archetype (followed by the warrior archetype). They also find that specific combinations of archetypes (including the warrior/ruler pairing) are more common in the top 100 songs. The below charts show some of these results.\n\n\n\nSource 6 Results - Intro Length\n\n\n\n\n\nSource 6 Results - Intro Length"
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "This page was used to track our progress and keep a log of our contributions to the project. We created this after developing our idea to create a running list of the workflow that was needed to complete the project and to demonstrate how each of us contributed to the project."
  },
  {
    "objectID": "technical-details/progress-log.html#satomi",
    "href": "technical-details/progress-log.html#satomi",
    "title": "Progress log",
    "section": "Satomi",
    "text": "Satomi\n\nprovide link to about me page\n\nContribution log:\n\nIndex.qmd Literature Review:\n\nFind 3 academic publications analyzing song popularity using data\nAnalyze publications\nReport Findings on Index.qmd page\n\n\n\nIndex.qmd:\n\nCreate about me page\n\n\n\nData Collection:\n\nDevelop code to access top 100 songs playlist from Rolling Stones top 100 songs\nDevelop code to access spotify api to get data about top 100 songs\nDevelop code to access spotify api to get data about artists of top 100 songs\nProvide comments about process and informative technical markdown cells\n\n\n\nData Cleansing:\n\nConsolodate all raw data csvs into one dataframe\nUse VADER to conduct sentiment analysis of lyrics for songs\nUse NLP methods to collect other data about lyrics of songs\nFind other quantitative data from columns for future analysis\nWrite all data to a csv file in a processed data folder\nProvide comments about process and informative technical markdown cells\n\n\n\nSupervised Learning:\n\nConduct classification on ranking of song (top 20 vs not)\nConduct regression on song popularity\nAnalyze results to determine important features contributing to success of song\nProvide comments about process and informative technical markdown cells\n\n\n\nReport:\n\nWrite about key findings from Supervised Learning sections\nProvide informative visualizations\nWrite conclusion\n\n\n\nOverall:\n\nFrequent github pushes\nFrequent github pull requests\nComment code and explain decisions in technical-details files"
  },
  {
    "objectID": "technical-details/progress-log.html#samyu",
    "href": "technical-details/progress-log.html#samyu",
    "title": "Progress log",
    "section": "Samyu",
    "text": "Samyu\n\nprovide link to about me page\n\nContribution log:\n\nIndex.qmd Literature Review:\n\nFind 3 academic publications analyzing song popularity using data\nAnalyze publications\nReport Findings on Index.qmd page\n\n\n\nIndex.qmd:\n\nCreate Landing page for website following instructions on page\nCreate about me page\n\n\n\nReport Introduction:\n\nWrite Objective\n\n\n\nData Collection:\n\nDevelop code to access collect lyrics for top 100 songs from Genius API\nProvide comments about process and informative technical markdown cells\n\n\n\nEDA:\n\nDevelop visualizations to analyze distributions of quantitative data\nConduct EDA to understand the characteristics of the most popular songs vs the rest of the songs\nCreate visualizations to show any correlations in the data\nProvide comments about process and informative technical markdown cells\n\n\n\nUnsupervised Learning:\n\nUse clustering algorithms to develop clusters based on quantitative data and understand connection to song ranking or popularity\nAnalyze results to determine the characteristics of popular songs\nDetermine if there are clusters that can then be used to make playlists of similar songs\nProvide comments about process and informative technical markdown cells\n\n\n\nReport:\n\nWrite executive summary\nWrite about key insights provided by EDA\nWrite about key insights provided by unsupervised learning\nProvide informative visualizations\nWrite about Business Implications of key findings\nWrite about Recommendations\n\n\n\nOverall:\n\nFrequent github pushes\nFrequent github pull requests\nComment code and explain decisions in technical-details files"
  }
]